{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: 217723.\n",
      "Loading RTE Dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c8e214c7e948c294cb367a6e11217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = '/dfs/scratch0/chami/metal/logs/2019_02_20/RTE_21_25_34/'\n",
    "task_name = 'RTE'\n",
    "split = 'dev'\n",
    "\n",
    "#Load model and data\n",
    "model,dl = load_data_and_model(model_path,task_name,split)\n",
    "\n",
    "#Create DataFrame of Raw Data, Predictions, and Labels\n",
    "print('creating dataframe')\n",
    "df_error = create_dataframe(task_name,model,dl)\n",
    "print('created dataframe')\n",
    "df_error.head()\n",
    "\n",
    "#Save (and reload) DataFrame\n",
    "csv_path = '/'.join(model_path.split('/')[0:-1])\n",
    "filepath = f'{task_name}_{split}_error_analysis.tsv'\n",
    "save_dataframe(df_error,filepath)\n",
    "df_error = load_dataframe(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox for Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Some basic statistics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import confusion_matrix\n",
    "#TODO: change to use the right function for label space change (0,1) to (1,2)\n",
    "confusion_matrix( df_error['label']+1.,1*(df_error['score']>0.5)+1., pretty_print=True)\n",
    "print()\n",
    "\n",
    "from metal.metrics import metric_score\n",
    "metric_list = ['accuracy','precision', 'recall', 'f1']\n",
    "\n",
    "for metric in metric_list:\n",
    "    score = metric_score(df_error['label']+1., 1*(df_error['score']>0.5)+1., metric, probs=df_error['score'])\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Predictions and Predicted Probabilistic Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.visualization.analysis import (\n",
    "        plot_predictions_histogram, \n",
    "        plot_probabilities_histogram,\n",
    "        plot_calibration_histogram\n",
    "    )\n",
    "plot_calibration_histogram(df_error['score'], df_error['label'], title=\"Probablistic Label Distribution\", legend=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.visualization.analysis import (\n",
    "        plot_predictions_histogram, \n",
    "        plot_probabilities_histogram,\n",
    "        plot_calibration_histogram\n",
    "    )\n",
    "\n",
    "plot_predictions_histogram((np.sign(df_error['score']-0.5)+1.)/2., df_error['label'], title=\"Label Distribution\")\n",
    "#plot_probabilities_histogram(df_error['score'], title=\"Probablistic Label Distribution\")\n",
    "plot_calibration_histogram(df_error['score'], df_error['label'], title=\"Probablistic Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. We want to look at examples that are \"barely\" wrong and \"barely\" right since we have hope for boosts here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mBARELY WRONG\\033[0;0m\")\n",
    "for i in range(3):\n",
    "    print_barely_pred(df_error,is_incorrect=True,thresh=0.2)\n",
    "    \n",
    "print(\"\\033[1mBARELY RIGHT\\033[0;0m\")\n",
    "for i in range(3):\n",
    "    print_barely_pred(df_error,is_incorrect=False,thresh=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We also want to look at examples we got completely wrong since that could point to a systematic bias in the data/model. It could also help us find examples in the dataset that are mislabeled by human annotators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mVERY WRONG\\033[0;0m\")\n",
    "for i in range(3):\n",
    "    print_very_wrong_pred(df_error,thresh=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. To find systematic errors, we can also look for correlations between certain features and the incorrectness a la Socratic**\n",
    "\n",
    "\n",
    "We can make this way more sophisticated by perhaps using embeddings instead of this simple [BoW featurization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mSYSTEMATIC EXAMPLES\\033[0;0m\")\n",
    "for i in range(3):\n",
    "    print_systematic_wrong(df_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Number Based LF**\n",
    "\n",
    "Our model tends to fail when there are numbers involved in the two sentences. We can look for the same number being repeated in both sentences as an LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_number(idx):\n",
    "    sentence1_nums = [int(s) for s in df_error['sentence1'][idx].split() if s.isdigit()]\n",
    "    sentence2_nums = [int(s) for s in df_error['sentence2'][idx].split() if s.isdigit()]\n",
    "    common_nums = len(set(sentence1_nums).intersection(set(sentence2_nums)))\n",
    "    \n",
    "    if (sentence1_nums == []) or (sentence2_nums == []):\n",
    "        return 0\n",
    "    \n",
    "    if common_nums > 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_row(df_error.iloc[70])\n",
    "print(\"LF_label: \", LF_number(70)-1)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print_row(df_error.iloc[254])\n",
    "print(\"LF_label: \", LF_number(254)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Edit Distance Based LF**\n",
    "\n",
    "Our model tends to vote entailment when one sentence is long and the other is short. We can focus on this slice and flip the labelt o vote no entailment even when the number of words is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "from collections import Counter\n",
    "def common_words(s1,s2):\n",
    "    s1_set = set(Counter(s1.split()))\n",
    "    s2_set = set(Counter(s2.split()))\n",
    "    return len(s1_set.intersection(s2_set))/float(min(len(s1_set),len(s2_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_words(idx):\n",
    "    ratio = common_words(df_error['sentence1'][idx], df_error['sentence2'][idx])\n",
    "    if ratio < 0.3:\n",
    "        return 1\n",
    "    if (ratio <= 1.0) and (ratio > 0.4):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_row(df_error.iloc[150])\n",
    "print(\"LF_label: \", LF_words(150)-1)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print_row(df_error.iloc[95])\n",
    "print(\"LF_label: \", LF_words(95)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.zeros((np.shape(df_error)[0],2))\n",
    "for i in range(df_error.shape[0]):\n",
    "    L[i,0] = LF_number(i)\n",
    "    L[i,1] = LF_words(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling Function Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import lf_summary\n",
    "from scipy.sparse import csr_matrix    \n",
    "\n",
    "L_sparse = csr_matrix(L)\n",
    "lf_summary(L_sparse,Y=df_error.label+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = set(np.where(df_error.is_wrong == True)[0])\n",
    "LF1_set = set(np.where(L[:,0]-1. == df_error.label)[0])\n",
    "LF2_set = set(np.where(L[:,1]-1. == df_error.label)[0])\n",
    "\n",
    "print(\"Percentage Corrected by LF_num: \", 100.*len(LF1_set.intersection(incorrect))/float(len(incorrect)))\n",
    "print(\"Percentage Corrected by LF_words: \", 100.*len(LF2_set.intersection(incorrect))/float(len(incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model for Task A to Predict on Train Set for Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with QNLI and RTE since those are both about `entailment` and `not_entailment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config (needs to be the same as parameters used for training)\n",
    "bert_model = \"bert-base-uncased\"\n",
    "max_len = 256\n",
    "bert_output_dim = 768\n",
    "max_datapoints = -1\n",
    "dl_kwargs = {\"batch_size\": 32, \"shuffle\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02/19/19\n",
    "model_paths = {\n",
    "    'MNLI_SAN': '/dfs/scratch1/senwu/mmtl/logs/checkpoints/17-2-2019/MNLI_SAN_02_27_41/',\n",
    "    'QNLI': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/QNLI_09_56_46/',\n",
    "    'STSB': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/STSB_09_46_46/',\n",
    "    'SST2': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/SST2_06_01_35/',\n",
    "    'COLA': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/COLA_05_49_39/',\n",
    "    'RTE':'/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/RTE_06_32_37/',\n",
    "    'WNLI': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/WNLI_06_38_32/',\n",
    "    'QQP': '/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/QQP_06_47_48/',\n",
    "    'MRPC':'/dfs/scratch0/mccreery/mmtl/logs/2019_02_19/MRPC_09_40_25/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_task_name = 'QNLI'\n",
    "target_task_name = 'RTE'\n",
    "\n",
    "#create source task\n",
    "source_task = create_tasks(\n",
    "    task_names=[source_task_name],\n",
    "    bert_model=bert_model,\n",
    "    max_len=max_len,\n",
    "    dl_kwargs=dl_kwargs,\n",
    "    splits=['test'],\n",
    "    max_datapoints=max_datapoints,\n",
    ")[0]\n",
    "\n",
    "# load source model weights \n",
    "source_model_path = os.path.join(model_paths[source_task_name], 'best_model.pth')\n",
    "source_model = MetalModel([source_task], verbose=False, device=0)\n",
    "source_model.load_weights(source_model_path)\n",
    "source_model.eval()\n",
    "    \n",
    "    \n",
    "#create target task\n",
    "target_task = create_tasks(\n",
    "    task_names=[target_task_name],\n",
    "    bert_model=bert_model,\n",
    "    max_len=max_len,\n",
    "    dl_kwargs=dl_kwargs,\n",
    "    splits=['train'],\n",
    "    max_datapoints=max_datapoints,\n",
    ")[0]\n",
    "\n",
    "# predict on target task train set\n",
    "target_task.name = source_task_name #HACK FOR LINE 225 in METAL_MODEL.PY\n",
    "Y, Y_probs, Y_preds = source_model._predict_probs(\n",
    "    target_task, split='train', return_preds=True)\n",
    "\n",
    "# true labels for target task train set\n",
    "Y_true = []\n",
    "for x, y in tqdm(list(target_task.data_loaders['train'])):\n",
    "    Y_true += list(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_idx = list(np.where(np.abs(Y_probs[:,0]-0.5) >= 0.4)[0])\n",
    "Y_true = np.array(Y_true)\n",
    "print (f'Accuracy of {source_task_name} model on {target_task_name}: {np.mean(Y_preds[confident_idx] == Y_true[confident_idx])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
