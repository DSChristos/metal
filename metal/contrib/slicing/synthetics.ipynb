{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(rpca_errs, amc_errs, idx, iters, vals_list):\n",
    "    \"\"\"\n",
    "    Plot a line graph with 95% CI shaded!\n",
    "    \"\"\"\n",
    "\n",
    "    #create pandas dataframe\n",
    "    m_list = vals_list\n",
    "    rpca_errs_dataframe = pd.DataFrame([obj[idx] for obj in rpca_errs])\n",
    "    rpca_errs_dataframe['m'] = pd.Series(m_list*iters, index=rpca_errs_dataframe.index)\n",
    "    amc_errs_dataframe = pd.DataFrame([obj[idx] for obj in amc_errs])\n",
    "    amc_errs_dataframe['m'] = pd.Series(m_list*iters, index=amc_errs_dataframe.index)\n",
    "\n",
    "    #create plot\n",
    "    sns.lineplot(x='m', y=0, data=rpca_errs_dataframe, marker=True, palette=sns.color_palette(\"GnBu_d\",2))\n",
    "    sns.lineplot(x='m', y=0, data=amc_errs_dataframe, marker=True, palette = sns.color_palette(\"GnBu_d\",2))\n",
    "    if idx == 6:\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title('Overall Accuracy')\n",
    "    if idx == 0:\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title('Head Slice Accuracy')\n",
    "    if idx == 3:\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title('Head-Torso Overlap Accuracy')\n",
    "\n",
    "    plt.xlabel(\"Overlap Portion of Head-Torso\")\n",
    "    plt.legend(['Model 0', 'Model 1'])\n",
    "    \n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,X,L,Y,overlap_idx):\n",
    "        #overall accuracy of the model\n",
    "        overall_score = model.score(X, Y) \n",
    "        \n",
    "        #accuracy of the model on the portion each LF covers\n",
    "        slice_scores = model.score_on_LF_slices(X, Y, L) \n",
    "        try: \n",
    "            #accuracy of the model on the overlap between head and torso LF\n",
    "            overlap_scores = model.score_on_LF_slices(X[overlap_idx,:], Y[overlap_idx], L[overlap_idx,:]) \n",
    "        except: \n",
    "            overlap_scores = [0,0,0] \n",
    "        return  [list(slice_scores)+list(overlap_scores)+list([overall_score])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def shuffle_matrices(matrices):\n",
    "    \"\"\"Shuffle each member of a list of matrices having the same first dimension\n",
    "    (along first dimension) according o the same shuffling order.\"\"\"\n",
    "    N = matrices[0].shape[0]\n",
    "    idxs = list(range(N))\n",
    "    shuffle(idxs)\n",
    "    out = []\n",
    "    for M in matrices:\n",
    "        if M.shape[0] != N:\n",
    "            raise ValueError(\"All matrices must have same first dimension.\")\n",
    "        out.append(M[idxs])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(mu_0, mu_1, n=1000, plot=False):\n",
    "    \"\"\"Generate (x, y) mixture of gaussians data, where x \\in R^d and \n",
    "    y \\in {-1, 1}.\"\"\"\n",
    "    dim = mu_0.shape[0]\n",
    "    nc = n // 2\n",
    "    I = np.diag(np.ones(dim))\n",
    "\n",
    "    # Generate data\n",
    "    X_0 = multivariate_normal(mu_0, I, size=nc)\n",
    "    X_1 = multivariate_normal(mu_1, I, size=nc)\n",
    "\n",
    "    # Plot the data\n",
    "    if plot:\n",
    "        plt.scatter(X_0[:,0], X_0[:,1])\n",
    "        plt.scatter(X_1[:,0], X_1[:,1])\n",
    "        plt.show()\n",
    "\n",
    "    # Shuffle and merge data\n",
    "    return shuffle_matrices([\n",
    "        np.vstack([X_0, X_1]),\n",
    "        np.array(np.concatenate([-np.ones(nc), np.ones(nc)]), dtype=int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_mode_data(n, mus, props, labels, plot=False):\n",
    "    \"\"\"Generate multi-mode data\n",
    "    \n",
    "    Args:\n",
    "        - n: [int] Number of data points to generate\n",
    "        - mus: [list of d-dim np.arrays] centers of the modes\n",
    "        - props: [list of floats] proportion of data in each mode\n",
    "        - labels: [list of ints] class label of each mode\n",
    "        - plot: [bool] Whether to plot the data\n",
    "    \n",
    "    Returns:\n",
    "        - X: [n x d-dim array] Data points\n",
    "        - Y: [n-dim array] Data labels\n",
    "        - C: [n-dim array] Index of the mode each data point belongs to\n",
    "    \"\"\"\n",
    "    assert sum(props) == 1.0\n",
    "    ns = [int(n*prop) for prop in props]\n",
    "    d = mus[0].shape[0]\n",
    "    I_d = np.diag(np.ones(d))\n",
    "\n",
    "    # Generate data\n",
    "    Xu = [np.random.multivariate_normal(mu, I_d, size=ni) for mu, ni in zip(mus, ns)]\n",
    "    Yu = [l*np.ones(ni) for ni, l in zip(ns, labels)]\n",
    "    Cu = [i*np.ones(ni) for i, ni in enumerate(ns)]\n",
    "\n",
    "    # Plot the data\n",
    "    if plot:\n",
    "        for Xi in Xu:\n",
    "            plt.scatter(Xi[:, 0], Xi[:, 1])\n",
    "        plt.show()\n",
    "\n",
    "    # Generate labels and shuffle\n",
    "    return shuffle_matrices([np.vstack(Xu), np.hstack(Yu), np.hstack(Cu)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_matrix(n, accs, covs, Y, C, overlap_portion=0.3, overlap_acc=1.0):\n",
    "    \"\"\"Generate label matrix. We assume that the last LF is the head LF and the \n",
    "    one before it is the torso LF it will interact with.\n",
    "    \n",
    "    Args:\n",
    "        - n: [int] Number of data points\n",
    "        - accs: [list of floats] accuracies of LFs\n",
    "        #TODO: covs isn't the overall coverage, but coverage on the associated mode\n",
    "        - covs: [list of floats] coverage for each LF for its mode\n",
    "        - Y: [n-dim array] Data labels\n",
    "        - C: [n-dim array] Index of the mode each data point belongs to\n",
    "        - overlap_portion: [float] % of \"head\" LF that overlaps with \"torso\" LF\n",
    "        TODO: Not using overlap_acc yet!\n",
    "        - overlap_acc: [float] Accuracy of torso LF | head LF on overlap_portion\n",
    "    \n",
    "    Returns:\n",
    "        - L: [n x d-dim array] Data points\n",
    "        - overlap_idx: [n-dim array] Index of where head and torso LF overlap\n",
    "    \"\"\"\n",
    "    m = np.shape(accs)[0]\n",
    "\n",
    "    # Construct a label matrix with given accs and covs\n",
    "    L = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        j = int(C[i])\n",
    "        if np.random.random() < covs[j]:\n",
    "            if np.random.random() < accs[j]:\n",
    "                L[i, j] = Y[i]\n",
    "            else:\n",
    "                L[i, j] = -Y[i]\n",
    "                \n",
    "    #Change labeling patterns of LF[-2] and LF[-1] so they have some overlap\n",
    "    for i in range(n):\n",
    "        j = int(C[i])\n",
    "        if j == int(np.max(C)):\n",
    "            if np.random.random() < overlap_portion:\n",
    "                L[i, j-1] = -Y[i] #downvote LF1 on overlap\n",
    "                L[i, j] = Y[i] #upvote LF2 on overlap\n",
    "\n",
    "    overlap_idx = [i for i in range(n) if (L[i,-2] != 0 and L[i,-1] != 0)]\n",
    "    accs_emp = np.array([np.mean(L[:,j] == Y)/np.mean(L[:,j] != 0) for j in range(m)])\n",
    "    return L, overlap_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N,accs,covs,overlap_portion=0.3):\n",
    "    \"\"\"\n",
    "    Generate simulation data with given params. \n",
    "    #TODO: HARDCODE LF[-2] and LF[-1] as head and torso LF\n",
    "    Args:\n",
    "        - N: [int] total number of datapoints (samples)\n",
    "        - accs: [list of floats] accuracies for LFs\n",
    "        - covs: [list of floats] coverages for LFs\n",
    "        - overlap_portion: [float] % of \"head\" LF that overlaps with \"torso\" LF\n",
    "    \"\"\"\n",
    "    #feature and label generation\n",
    "    mus = [\n",
    "    np.array([-3, 0]), # Mode 1: Y = -1\n",
    "    np.array([3, 0]), # Mode 2: Y = 1\n",
    "    np.array([6, -3]) # Mode 3: Y = -1\n",
    "    ]\n",
    "    props = [0.25, 0.5, 0.25]\n",
    "    labels = [-1, 1, -1]\n",
    "    \n",
    "    X, Y, C = generate_multi_mode_data(N, mus, props, \n",
    "                                               labels, plot=False)\n",
    "    \n",
    "    #labeling function generation\n",
    "    L, overlap_idx = generate_label_matrix(N, accs, covs, \n",
    "                                           Y, C, overlap_portion=overlap_portion)\n",
    "    \n",
    "    return X,Y,L,overlap_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "covs = np.array([0.9, 0.9, 0.9])\n",
    "m = np.shape(covs)[0]\n",
    "model_0_scores = []\n",
    "model_1_scores = []\n",
    "model_2_scores = []\n",
    "\n",
    "overlap_portion = 0.05\n",
    "accs = np.array([0.75, 0.75, 0.75])\n",
    "X,Y,L,overlap_idx = generate_data(N,accs,covs,overlap_portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X,L, accs):\n",
    "    \"\"\"\n",
    "    Trains baseline, oracle, and attention model\n",
    "    Args:\n",
    "        - X: features\n",
    "        - L: LF matrix\n",
    "        - accs: [list of floats] accuracies for LFs\n",
    "    Returns:\n",
    "        - model_[0,1,2]: trained baseline, oracle, and attention model\n",
    "    \"\"\"\n",
    "\n",
    "    m = np.shape(L)[1] #num LFs\n",
    "    d = X.shape[1] #num features\n",
    "\n",
    "    #baseline model, no attention\n",
    "    model_0 = SliceDPModel(d, LinearModule, m, accs, r=4, rw=False)\n",
    "    model_0.train(X, L, batch_size=1000, n_epochs=250, lr=0.1, print_every=250)\n",
    "\n",
    "    #oracle, manual reweighting\n",
    "    #TODO: currently hardcode weights so LF[-1] has double the weight\n",
    "    weights = np.ones(m)\n",
    "    weights[-1] = 2.0\n",
    "    model_1 = SliceDPModel(d, LinearModule, m, accs, r=4, rw=True, L_weights=list(weights))\n",
    "    model_1.train(X, L, batch_size=1000, n_epochs=250, lr=0.1, print_every=250)\n",
    "\n",
    "    #our model, with attention\n",
    "    model_2 = SliceDPModel(d, LinearModule, m, accs, r=2, rw=True)\n",
    "    model_2.train(X, L, batch_size=1000, n_epochs=250, lr=0.1, print_every=250)\n",
    "    \n",
    "    return model_0, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, bias=False):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.input_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def predict(self, x):\n",
    "        yp = self.predict_proba(x).squeeze().detach().numpy()\n",
    "        return np.where(yp > 0.5, 1, -1)\n",
    "\n",
    "    def score(self, X_np, Y_np):\n",
    "        X = self._convert_np_data(X_np)\n",
    "        return np.where(self.predict(X) == Y_np, 1, 0).sum() / X.shape[0]\n",
    "    \n",
    "    def score_on_LF_slices(self, X_np, Y_np, L_np):\n",
    "        \"\"\"Return the score for each coverage set of each LF\"\"\"\n",
    "        m = L_np.shape[1]\n",
    "        Yp = np.tile(self.predict(self._convert_np_data(X_np)), (m, 1))\n",
    "        Yp = np.abs(L_np).T * Yp\n",
    "        return 0.5 * (Yp @ Y_np / np.sum(np.abs(L_np), axis=0) + 1)\n",
    "    \n",
    "    def train(self, X_np, L_np, batch_size=10, n_epochs=10, lr=0.01, \n",
    "        momentum=0.9, print_every=10):\n",
    "        \"\"\"Train a standard supervised model using SGD with momentum.\"\"\"\n",
    "        X, L = map(self._convert_np_data, [X_np, L_np])\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_loader = DataLoader(TensorDataset(X, L), batch_size=batch_size)\n",
    "\n",
    "        # Set optimizer as SGD w/ momentum\n",
    "        optimizer = optim.SGD(self.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        # Train model\n",
    "        for epoch in range(n_epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch, data in enumerate(train_loader):\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward + backward + optimize\n",
    "                loss = self.loss(*data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.detach()\n",
    "            \n",
    "            # Print loss every 10 epochs\n",
    "            if epoch % print_every == 0 or epoch == n_epochs - 1:\n",
    "                avg_loss = running_loss / batch\n",
    "                print(f\"[Epoch {epoch}] Loss: {avg_loss:0.3f}\")\n",
    "        \n",
    "        print('Finished Training')\n",
    "    \n",
    "    def _convert_np_data(self, X_np, convert_binary=False):\n",
    "        X_np = np.copy(X_np)\n",
    "\n",
    "        # Optionally: Convert from {-1,1} --> {0,1}\n",
    "        if convert_binary:\n",
    "            X_np[X_np == -1] = 0\n",
    "\n",
    "        return torch.from_numpy(X_np).float()\n",
    "    \n",
    "    def print_params(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            print(\"\\n\", name, param)\n",
    "\n",
    "class SliceDPModel(Classifier):\n",
    "    def __init__(self, input_dim, input_module_class, m, accs, r=1, rw=False, L_weights=[]):\n",
    "        \"\"\"Online / joint data programming model\n",
    "        Assumes balanced, binary class problem with conditionally ind. LFs that\n",
    "        output binary labels or abstain, \\lambda_i \\in {-1,0,1}\n",
    "        Args:\n",
    "            - input_dim: Input data vector dimension\n",
    "            - input_module_class: Class that initializes with args (input_dim,\n",
    "                output_dim)\n",
    "            - m: Number of label sources\n",
    "            - accs: The LF accuracies, computed offline\n",
    "            - r: Intermediate representation dimension\n",
    "            - rw: Whether to use reweighting of representation for Y_head\n",
    "            - L_weights: If provided, manually weight L_heads using L_weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.k = 1 # Fixed here for binary setting\n",
    "        self.m = m\n",
    "        self.r = r\n",
    "        self.rw = rw\n",
    "        self.L_weights = torch.from_numpy(np.array(L_weights, dtype=np.float32))\n",
    "\n",
    "        # Basic binary loss function\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduce=False)\n",
    "\n",
    "        # Input module\n",
    "        self.input_layer = nn.Sequential(\n",
    "            input_module_class(input_dim, self.r),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Attach an [r, m] linear layer to predict the labels of the LFs\n",
    "        self.L_head = nn.Linear(self.r, self.m, bias=False)\n",
    "\n",
    "        # Attach the \"DP head\" which outputs the final prediction\n",
    "        y_d = 2 * self.r if self.rw else self.r\n",
    "        self.Y_head = nn.Linear(y_d, self.k, bias=False)\n",
    "\n",
    "        # Start by getting the DP marginal probability of Y=1, using the\n",
    "        # provided LF accuracies, accs, and assuming cond. ind., binary LFs\n",
    "        self.w = torch.from_numpy(np.log(accs / (1-accs))).float()\n",
    "\n",
    "    def forward_L(self, x):\n",
    "        \"\"\"Returns the unnormalized predictions of the L_head layer.\"\"\"\n",
    "        return self.L_head(self.input_layer(x))\n",
    "    \n",
    "    def forward_Y(self, x):\n",
    "        \"\"\"Returns the output of the Y head only, over re-weighted repr.\"\"\"\n",
    "        b = x.shape[0]\n",
    "        xr = self.input_layer(x)\n",
    "\n",
    "        # Concatenate with the LF attention-weighted representation as well\n",
    "        if self.rw:\n",
    "\n",
    "            # A is the [bach_size, 1, m] Tensor representing the relative\n",
    "            # \"confidence\" of each LF on each example\n",
    "            # NOTE: Should we be taking an absolute value / centering somewhere\n",
    "            # before here to capture the \"confidence\" vs. prediction...?\n",
    "            A = F.softmax(self.forward_L(x)).unsqueeze(1)\n",
    "\n",
    "            # We then project the A weighting onto the respective features of\n",
    "            # the L_head layer, and add these attention-weighted features to Xr\n",
    "            if self.L_weights.shape[0] > 0:\n",
    "                # Manually reweight\n",
    "                W = self.L_weights.repeat(4,1).transpose(0,1).repeat(b, 1, 1)\n",
    "            else:\n",
    "                # Use learned weights from L_head\n",
    "                W = self.L_head.weight.repeat(b, 1, 1)\n",
    "\n",
    "            xr = torch.cat([xr, torch.bmm(A, W).squeeze()], 1)\n",
    "\n",
    "        # Return the list of head outputs + DP head\n",
    "        return self.Y_head(xr).squeeze()\n",
    "    \n",
    "    def loss(self, x, L):\n",
    "        \"\"\"Returns the loss consisting of summing the LF + DP head losses\n",
    "        Args:\n",
    "            - x: A [batch_size, d] torch Tensor\n",
    "            - L: A [batch_size, m] torch Tensor with elements in {-1,0,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert label matrix to [0,1] scale, and create abstain mask\n",
    "        L_01 = (L + 1) / 2\n",
    "        L_mask = torch.abs(L_01)\n",
    "        nb = torch.sum(L_mask)\n",
    "        \n",
    "        # LF heads loss\n",
    "        # NOTE: Here, we only add non-abstains to the loss\n",
    "        # loss_1 = torch.sum(self.loss_fn(self.forward_L(x), L_01) * L_mask) / nb\n",
    "        # NOTE: Here, we add *all* data points to the loss\n",
    "        loss_1 = torch.mean(self.loss_fn(self.forward_L(x), L_01))\n",
    "\n",
    "        # Compute the noise-aware DP loss w/ the reweighted representation\n",
    "        # Note: Need to convert L from {0,1} --> {-1,1}\n",
    "        loss_2 = torch.mean(\n",
    "            self.loss_fn(self.forward_Y(x), F.sigmoid(2 * L @ self.w))\n",
    "        )\n",
    "\n",
    "        # Just take the unweighted sum of these for now...\n",
    "        return (10*loss_1 + loss_2) / 2\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return F.sigmoid(self.forward_Y(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss: 4.177\n",
      "[Epoch 249] Loss: 4.012\n",
      "Finished Training\n",
      "[Epoch 0] Loss: 4.245\n",
      "[Epoch 249] Loss: 4.011\n",
      "Finished Training\n",
      "[Epoch 0] Loss: 4.316\n",
      "[Epoch 249] Loss: 4.061\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "covs = np.array([0.9, 0.9, 0.9])\n",
    "m = np.shape(covs)[0]\n",
    "model_0_scores = []\n",
    "model_1_scores = []\n",
    "model_2_scores = []\n",
    "\n",
    "overlap_portion = 0.05\n",
    "accs = np.array([0.75, 0.75, 0.75])\n",
    "X,Y,L,overlap_idx = generate_data(N,accs,covs,overlap_portion)\n",
    "\n",
    "#train the models and score the models (I KNOW ITS UGLY)\n",
    "model_0, model_1, model_2 = train_models(X,L,accs)\n",
    "for model, m_scores in zip([model_0, model_1, model_2],[model_0_scores, model_1_scores, model_2_scores]):\n",
    "    a,b,c,d,e,f,g = eval_model(model,X,L,Y,overlap_idx)\n",
    "    m_scores.append([a,b,c,d,e,f,g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LF0_slice</th>\n",
       "      <th>LF1_slice</th>\n",
       "      <th>LF2_slice</th>\n",
       "      <th>LF1+2_overlap</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.995111</td>\n",
       "      <td>0.985046</td>\n",
       "      <td>0.966578</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manual</th>\n",
       "      <td>0.995556</td>\n",
       "      <td>0.985262</td>\n",
       "      <td>0.969251</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ours</th>\n",
       "      <td>0.999556</td>\n",
       "      <td>0.983962</td>\n",
       "      <td>0.976827</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.9861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LF0_slice  LF1_slice  LF2_slice  LF1+2_overlap  Overall\n",
       "Baseline   0.995111   0.985046   0.966578       0.968254   0.9833\n",
       "Manual     0.995556   0.985262   0.969251       0.968254   0.9842\n",
       "Ours       0.999556   0.983962   0.976827       0.976190   0.9861"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = pd.DataFrame([[model_0_scores[0][i] for i in [0,1,2,5,6]], [model_1_scores[0][i] for i in [0,1,2,5,6]], [model_2_scores[0][i] for i in [0,1,2,5,6]]])\n",
    "scores.columns = (['LF0_slice', 'LF1_slice', 'LF2_slice', 'LF1+2_overlap', 'Overall'])\n",
    "scores.index = (['Baseline', 'Manual', 'Ours'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
