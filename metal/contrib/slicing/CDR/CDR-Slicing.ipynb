{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing CDR Relation Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/dfs/scratch0/vschen/metal')\n",
    "\n",
    "import metal\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:  0.4.1\n",
      "MeTaL:    0.3.3\n",
      "Python:   3.6.7 (default, Dec  8 2018, 17:35:14) \n",
      "[GCC 5.4.0 20160609]\n",
      "Python:   sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch: ', torch.__version__)\n",
    "print('MeTaL:   ', metal.__version__)\n",
    "print('Python:  ', sys.version)\n",
    "print('Python:  ', sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalize CDR Dataset\n",
    "To uncompress the SQLite db: ```bzip2 -d cdr.db.bz2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to sqlite:////dfs/scratch0/vschen/metal/metal/contrib/slicing/CDR/cdr.db\n",
      "Connected to sqlite:////dfs/scratch0/vschen/metal/metal/contrib/slicing/CDR/cdr.db\n",
      "Connected to sqlite:////dfs/scratch0/vschen/metal/metal/contrib/slicing/CDR/cdr.db\n",
      "[TRAIN] 8272\n",
      "[DEV]   888\n",
      "[TEST]  4620\n"
     ]
    }
   ],
   "source": [
    "from metal.contrib.backends.wrapper import SnorkelDataset\n",
    "import os\n",
    "\n",
    "db_conn_str   = os.path.join(os.getcwd(),\"cdr.db\")\n",
    "candidate_def = ['ChemicalDisease', ['chemical', 'disease']]\n",
    "\n",
    "train, dev, test = SnorkelDataset.splits(db_conn_str, \n",
    "                                         candidate_def, \n",
    "                                         max_seq_len=125)\n",
    "\n",
    "print(f'[TRAIN] {len(train)}')\n",
    "print(f'[DEV]   {len(dev)}')\n",
    "print(f'[TEST]  {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pretrained Embeddings\n",
    "\n",
    "Download [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip):\n",
    "`wget http://nlp.stanford.edu/data/glove.6B.zip \\\n",
    "&& mkdir -p glove.6B \\\n",
    "&& unzip glove.6B.zip -d glove.6B \\\n",
    "&& rm glove.6B.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import EmbeddingLoader, load_embeddings\n",
    "emb_path  = \"../glove.6B/glove.6B.50d.txt\"\n",
    "embs  = EmbeddingLoader(emb_path, fmt='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate `L_*` to target slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LF_c_cause_d', 'LF_c_d', 'LF_c_induced_d', 'LF_c_treat_d', 'LF_c_treat_d_wide', 'LF_closer_chem', 'LF_closer_dis', 'LF_ctd_marker_c_d', 'LF_ctd_marker_induce', 'LF_ctd_therapy_treat', 'LF_ctd_unspecified_treat', 'LF_ctd_unspecified_induce', 'LF_d_following_c', 'LF_d_induced_by_c', 'LF_d_induced_by_c_tight', 'LF_d_treat_c', 'LF_develop_d_following_c', 'LF_far_c_d', 'LF_far_d_c', 'LF_improve_before_disease', 'LF_in_ctd_therapy', 'LF_in_ctd_marker', 'LF_in_patient_with', 'LF_induce', 'LF_induce_name', 'LF_induced_other', 'LF_level', 'LF_measure', 'LF_neg_d', 'LF_risk_d', 'LF_treat_d', 'LF_uncertain', 'LF_weak_assertions']\n"
     ]
    }
   ],
   "source": [
    "from labeling_functions import LFs\n",
    "print ([lf.__name__ for lf in LFs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/8272 [00:00<02:51, 48.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2665/8272 [00:21<01:01, 90.96it/s] "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "L_train = labeler.apply(split=0)\n",
    "L_dev = labeler.apply(split=1) # used for debugging\n",
    "L_test = labeler.apply(split=2) # used for evaluation\n",
    "\n",
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "# need to extract `accs` from gen_model\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=0.0\n",
    ")\n",
    "\n",
    "accs = np.array(gen_model.learned_lf_stats()['Accuracy'])\n",
    "accs[np.isnan(accs)] = 0\n",
    "accs = np.minimum(accs, 0.999)\n",
    "\n",
    "gen_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L_train.copy()\n",
    "L[L==-1] = 2 # convert to multiclass\n",
    "Y_dev = np.array([ex[1] for ex in dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "label_model = LabelModel(k=2, seed=123)\n",
    "label_model.train_model(L, Y_dev=Y_dev)\n",
    "label_model.score((L_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Labels in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_marginals = label_model.predict_proba(L)\n",
    "metal_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snorkel_marginals = np.vstack((gen_marginals, 1-gen_marginals)).T\n",
    "snorkel_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.slicing.sqlite_wrapper \\\n",
    "    import SnorkelDataset as SnorkelSliceDataset\n",
    "\n",
    "train_metal = SnorkelSliceDataset(\n",
    "    db_conn_str,\n",
    "    candidate_def,\n",
    "    split=0,\n",
    "    train_marginals=metal_marginals\n",
    ")\n",
    "\n",
    "train_snorkel = SnorkelSliceDataset(\n",
    "    db_conn_str,\n",
    "    candidate_def,\n",
    "    split=0,\n",
    "    train_marginals=snorkel_marginals\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Slicing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = SnorkelSliceDataset(\n",
    "    db_conn_str,\n",
    "    candidate_def,\n",
    "    split=0,\n",
    "    L_train=L_train.todense()\n",
    ")\n",
    "\n",
    "train_slice_metal = SnorkelSliceDataset(\n",
    "    db_conn_str,\n",
    "    candidate_def,\n",
    "    split=0,\n",
    "    L_train=L_train.todense(),\n",
    "    train_marginals=metal_marginals\n",
    ")\n",
    "\n",
    "train_slice_snorkel = SnorkelSliceDataset(\n",
    "    db_conn_str,\n",
    "    candidate_def,\n",
    "    split=0,\n",
    "    L_train=L_train.todense(),\n",
    "    train_marginals=snorkel_marginals\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) `Oracle`: EndModel Trained on Full GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.end_model import EndModel\n",
    "from metal.modules import LSTMModule\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100, \n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0, \n",
    "                  num_layers=1, \n",
    "                  freeze=False)\n",
    "\n",
    "oracle = EndModel([200, 2], input_module=lstm, seed=123, use_cuda=use_cuda)\n",
    "oracle.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "oracle.config['train_config']['validation_metric'] = 'f1'\n",
    "oracle.config['train_config']['batch_size'] = 32\n",
    "oracle.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time oracle.train_model(train, dev_data=dev)\n",
    "oracle.score(test, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) `BaseWeak`: EndModel trained on weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.end_model import EndModel\n",
    "from metal.modules import LSTMModule\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100,\n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0,\n",
    "                  num_layers=1,\n",
    "                  freeze=False)\n",
    "\n",
    "base_weak = EndModel([200, 2], input_module=lstm, seed=123, use_cuda=use_cuda)\n",
    "\n",
    "base_weak.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "base_weak.config['train_config']['validation_metric'] = 'f1'\n",
    "base_weak.config['train_config']['batch_size'] = 32\n",
    "base_weak.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time base_weak.train_model(train_snorkel, dev_data=dev)\n",
    "base_weak_scores = base_weak.score(test, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) `SliceUW`: Unweighted SliceModel with `rw=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.slicing.online_dp import SliceDPModel, LinearModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100, \n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0, \n",
    "                  num_layers=1, \n",
    "                  freeze=False)\n",
    "\n",
    "r_dim = 200\n",
    "rw = False\n",
    "slice_uw = SliceDPModel(lstm, accs, r_dim, rw, seed=123, use_cuda=True)\n",
    "\n",
    "slice_uw.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "slice_uw.config['train_config']['validation_metric'] = 'f1'\n",
    "slice_uw.config['train_config']['batch_size'] = 32\n",
    "slice_uw.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time slice_uw.train_model(train_slice, dev_data=dev)\n",
    "slice_uw_scores = slice_uw.score(test, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) `SliceOurs`: Attention SliceModel with `rw=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100, \n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0, \n",
    "                  num_layers=1, \n",
    "                  freeze=False)\n",
    "\n",
    "\n",
    "r_dim = 200\n",
    "rw = True\n",
    "slice_ours = SliceDPModel(lstm, accs, r_dim, rw, seed=123, use_cuda=True)\n",
    "\n",
    "slice_ours.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "slice_ours.config['train_config']['validation_metric'] = 'f1'\n",
    "slice_ours.config['train_config']['batch_size'] = 32\n",
    "slice_ours.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time slice_ours.train_model(train_slice, dev_data=dev)\n",
    "slice_ours_scores = slice_ours.score(test, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) `SliceOursWeak`: Slice Model with $\\tilde{Y}$ priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from metal.contrib.slicing.online_dp import SliceDPModel, LinearModule\n",
    "from metal.modules import LSTMModule\n",
    "\n",
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100, \n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0, \n",
    "                  num_layers=1, \n",
    "                  freeze=False)\n",
    "\n",
    "r_dim = 200\n",
    "rw = True\n",
    "slice_ours_weak = SliceDPModel(lstm, accs, r_dim, rw, seed=123, use_cuda=True)\n",
    "\n",
    "slice_ours_weak.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "slice_ours_weak.config['train_config']['validation_metric'] = 'f1'\n",
    "slice_ours_weak.config['train_config']['batch_size'] = 32\n",
    "slice_ours_weak.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time slice_ours_weak.train_model(train_slice_snorkel, dev_data=dev)\n",
    "slice_ours_weak_scores = slice_ours_weak.score(test, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) `SliceUWWeak`: Unweighted Slice model with $\\tilde{Y}$ priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.slicing.online_dp import SliceDPModel, LinearModule\n",
    "from metal.modules import LSTMModule\n",
    "\n",
    "wembs = load_embeddings(train.word_dict, embs)\n",
    "lstm = LSTMModule(embed_size=50, \n",
    "                  hidden_size=100, \n",
    "                  embeddings=wembs,\n",
    "                  lstm_reduction='attention', \n",
    "                  dropout=0, \n",
    "                  num_layers=1, \n",
    "                  freeze=False)\n",
    "\n",
    "\n",
    "r_dim = 200\n",
    "rw = False\n",
    "slice_uw_weak = SliceDPModel(lstm, accs, r_dim, rw, seed=123, use_cuda=True)\n",
    "\n",
    "slice_uw_weak.config['train_config']['optimizer_config']['optimizer_common']['lr'] = 0.01\n",
    "slice_uw_weak.config['train_config']['validation_metric'] = 'f1'\n",
    "slice_uw_weak.config['train_config']['batch_size'] = 32\n",
    "slice_uw_weak.config['train_config']['n_epochs'] = 10\n",
    "\n",
    "%time slice_uw_weak.train_model(train_slice_snorkel, dev_data=dev)\n",
    "slice_uw_weak_scores = slice_uw_weak.score(test, metric=['precision', 'recall', 'f1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice-specific scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: don't call private fns\n",
    "Yp_oracle, Y = oracle._get_predictions(test)\n",
    "Yp_base_weak, Y = base_weak._get_predictions(test)\n",
    "Yp_slice_uw, Y = slice_uw._get_predictions(test)\n",
    "Yp_slice_ours, Y = slice_ours._get_predictions(test)\n",
    "Yp_slice_ours_weak, Y = slice_ours_weak._get_predictions(test)\n",
    "Yp_slice_uw_weak, Y = slice_uw_weak._get_predictions(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `slice_ours` (re-weighting, accuracy priors) vs. `base_weak` (end_model trained on weak labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.slicing.experiment_utils import compare_LF_slices\n",
    "compare_LF_slices(Yp_slice_ours, Yp_base_weak, \n",
    "                  Y, L_test, LFs, metric='accuracy', delta_threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `slice_ours_weak` (slice model with weak priors + reweighting) vs. `base_weak` (end_model trained on weak labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_LF_slices(Yp_slice_ours_weak, Yp_base_weak,\n",
    "                  Y, L_test, LFs, metric='accuracy', delta_threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `slice_ours_weak` vs. `oracle` (trained on full GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_LF_slices(Yp_slice_ours, Yp_oracle,\n",
    "                  Y, L_test, LFs, metric='accuracy', delta_threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `slice_ours` vs. `Yp_slice_uw` (unweighted slice model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_LF_slices(Yp_slice_ours, Yp_slice_uw,\n",
    "                  Y, L_test, LFs, metric='accuracy', delta_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
