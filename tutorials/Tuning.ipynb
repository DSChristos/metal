{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models often have many hyperparameters that need to be tuned to achieve maximal performance (e.g: learning rate, dropout rate, number of layers, layer size, etc) . This motivates the need for hyperparameter tuners that intelligently search the space of hyperparameters for a high performing model. \n",
    "\n",
    "To address this, MeTaL supports multiple hyperparameter tuners with an easy-to-use interface that allows users to streamline the hyperparameter optimization process. This tutorial covers utilizing MeTaL's hyperparameter tuners to tune an EndModel for maximal performance. Currently, two hyperparameter algorithms are supported:\n",
    "\n",
    "- <b>Random Search</b>\n",
    "- <b>Hyperband</b>\n",
    "\n",
    "The tutorial is broken down into the following sections \n",
    "\n",
    "1. <b>Set up the Problem and Load the Data</b>\n",
    "2. <b>Define the Search Space</b>\n",
    "3. <b>Run Random Search</b>\n",
    "4. <b>Run Hyperband Search</b>\n",
    "5. <b>Compare Random Search against Hyperband Search</b>\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before beginning, we first need to make sure that the metal/ directory is on our Python path. If the following cell runs without an error, you're all set. If not, make sure that you've installed snorkel-metal with pip or conda (or that you've added the repo to your path if you're running from source; for example, running source add_to_path.sh from the repository root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import metal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Problem and Load the Data\n",
    "\n",
    "First let's set up our problem and load our data. For the purposes of this tutorial (and to keep the search process short) we will use the small model introduced in the basic tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic tutorial data\n",
    "from metal.utils import split_data\n",
    "import pickle\n",
    "\n",
    "with open(\"data/basics_tutorial.pkl\", 'rb') as f:\n",
    "    X, Y, L, D = pickle.load(f)\n",
    "    \n",
    "Xs, Ys, Ls, Ds = split_data(X, Y, L, D, splits=[0.8, 0.1, 0.1], stratify_by=Y, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and train our label model like in the basic tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a the label model\n",
    "from metal.label_model import LabelModel\n",
    "label_model = LabelModel(k=2, seed=123)\n",
    "\n",
    "label_model.train_model(Ls[0], Y_dev=Ys[1], n_epochs=1000, log_train_every=250, lr=0.01, l2=1e-1)\n",
    "print('\\nTrained Label Model:')\n",
    "scores = label_model.score((Ls[1], Ys[1]), metric=['accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "print('\\nMajority Label Voter:')\n",
    "scores = mv.score((Ls[1], Ys[1]), metric=['accuracy', 'precision', 'recall', 'f1'])\n",
    "Y_train_ps = label_model.predict_proba(Ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our EndModel and verify that it successfully runs and achieves a decent score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an end model\n",
    "from metal.end_model import EndModel\n",
    "\n",
    "end_model_basic = EndModel([1000,2], \n",
    "                           train_config={\n",
    "                               \"n_epochs\": 5,\n",
    "                               \"l2\" : .1,\n",
    "                               \"validation_metric\":'f1',\n",
    "                               \"data_loader_config\" : {\n",
    "                                   \"batch_size\" : 256\n",
    "                               }\n",
    "                           }, seed=123)\n",
    "\n",
    "end_model_basic.train_model((Xs[0], Y_train_ps), valid_data=(Xs[1], Ys[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Notice that our F1 is around .953. In the sections below we will try to optimize the hyperparameters of this EndModel to achieve an even higher score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Search Space\n",
    "\n",
    "Before starting the hyperparameter tuning process, we need to specify the space of the hyperparameters we're searching. \n",
    "\n",
    "For the purposes of this tutorial we search over the following hyperparameters:\n",
    "- <b>n_epochs</b>: Integer representing the number of epochs to train\n",
    "- <b>batchnorm</b>: Boolean representing whether to use batch-normalization\n",
    "- <b>lr</b>: Float representing the learning rate for optimization\n",
    "- <b>layer_out_dims</b>: The architecture of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'seed' : [123],\n",
    "    'n_epochs': [1, 5, 10],\n",
    "    'batchnorm' : [True, False],\n",
    "    'dropout': [0, .1, .2, .3, .4, .5],\n",
    "    'lr': {'range': [1e-5, 1], 'scale': 'log'},\n",
    "    'layer_out_dims' : [[1000,10,2], [1000, 100, 2]],\n",
    "    'log_train_every': 5,\n",
    "    'log_valid_every': 5,\n",
    "    'data_loader_config': [{\"batch_size\": 256, \"num_workers\": 1}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of what each line in the configuration means:\n",
    "\n",
    "- `'seed': [123],`: This specifies that each end model should be initialized with the same random seed (for repeatability)\n",
    "- `'n_epochs': [1, 5, 10],`: This specifies that the hyperparameter tuner may train the model for either 1, 5 or 10 epochs\n",
    "- `'batchnorm' : [True, False],`: This specifies that a model instantiated by the tuner may have batchnorm as either True or False\n",
    "- `dropout': [0, .1, .2, .3, .4, .5]`: Like the above, this specifies that the dropout parameter of an instantiated model may be one of 0, .1, .2, .3, .4, or .5\n",
    "- `'lr': {'range': [1e-5, 1], 'scale': 'log'}`: This specifies that the learning rate of the training of a model may range from 1e-5 to 1, and that the tuner samples the learning rate on a log scale\n",
    "- `'layer_out_dims' : [[1000,10,2], [1000, 100, 2]]`: This specifies that upon instantiation of the model, the structure of the fully connected network can either be [1000, 10, 2] or [1000, 100, 2]; in the latter case, this means the network takes a 1000 dimensional input, has a hidden layer with 100 features and an output layer with 2 classes\n",
    "- `'log_train_every': 5`: This specifies that the model should print status updates every 5 iterations of training.\n",
    "- `'log_valid_every': 5`: This specifies that the model should print calculate metrics for the validation set every 5 iterations of training.\n",
    "- `'data_loader_config': [{\"batch_size\": 256, \"num_workers\": 1}],`: This specifies to use a batch of 256 for optimization and a single worker for loading the data\n",
    "\n",
    "Now that our search space is defined, let's start optimizing hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, random search has proven to be a powerful and efficient algorithm for tuning hyperparameters (see http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf for why). Let's use the RandomSearch tuner to find a good set of hyperparameters for our EndModel. Note that although we only do hyperparameter optimization for the EndModel, the tuners may also be used to do hyperparameter optimization for LabelModels.\n",
    "\n",
    "To start, let's import the RandomSearchTuner and instantiate our RandomSearchTuner to optimize an EndModel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.tuners.random_tuner import RandomSearchTuner\n",
    "rs_tuner = RandomSearchTuner(EndModel, seed=123, validation_metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = [(Xs[0], Y_train_ps)]\n",
    "X_dev, Y_dev = Xs[1], Ys[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we're prepped to launch our random search! Performing the search is just as easy and requires just a single call to the `search` function.\n",
    "\n",
    "Most of the arguments to the `search` function below are self explanatory, but there are a couple of key arguments to watch out for:\n",
    "- `max_search` : This specifies the number of configurations to search over. As it is set to 20 below, this means we search over 20 random models and return the best one\n",
    "- `verbose`: This specifies whether the tuner should be verbose or not and can be used to turn on/off the its logging feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_rs_model = rs_tuner.search(search_space, (X_dev, Y_dev), train_args=train_args, max_search=20, verbose=False, seed=123)\n",
    "rs_tuner.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best random search model achieves an F1 of ~.975 which outperforms the model we had previously (F1 ~ .953). Can we do even better than random search by either attaining the same accuracy faster or achieving a higher score? The following section walks through using the <b>Hyperband</b> tuner, which recent research has shown to be more efficient than random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperband Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While random search performs surprisingly well, we can be more efficient if we adaptively allocate more compute resources for configurations that perform well than to those that don't. For example if a configuration seems to yield a really poor model after the first epoch of training, it's unlikely it'll perform well even after more training, so we can early-terminate the training of this configuration to save compute. This is the core idea behind the <b>Hyperband</b> algorithm which recent research has shown to outperform various algorithms including random search. (See https://arxiv.org/abs/1603.06560 if interested!)\n",
    "\n",
    "Running Hyperband is just as easy as running random search. Let's import the HyperbandTuner and instantiate it. \n",
    "\n",
    "Note that there is one extra argument to initialize the HyperbandTuner:\n",
    "- `hyperband_epochs_budget`: This specifies the number of total epochs of training the tuner can perform in its search for a performant model. This is used to create the Hyperband search schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.tuners.hyperband_tuner import HyperbandTuner\n",
    "hb_tuner = HyperbandTuner(EndModel, hyperband_epochs_budget=200, seed=123, validation_metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch the Hyperband search process using the same `search` call. Note that since the Hyperband schedule already limits the amount of compute we do, we don't have to set the `max_search` argument; the algorithm will attempt to make best use of the computed we've allocated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hb_model = hb_tuner.search(search_space, (X_dev, Y_dev), train_args=train_args, verbose=False, seed=123)\n",
    "hb_tuner.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve F1 ~.987, which beat our initial score of F1 ~.95 and essentially matches the score achieved by random search (F1 ~.975). The next section will compare the performances of random search and Hyperband using the logged data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Random Search against Hyperband Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During `search`, MeTaL hyperparameter tuners track useful statistics. Currently these include\n",
    "* Time elapsed\n",
    "* Achieved score\n",
    "* Configuration for score\n",
    "\n",
    "We will analyze these statistics to compare random search against Hyperband. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract the statistics captured by our tuners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_stats = hb_tuner.run_stats\n",
    "rs_stats = rs_tuner.run_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few of the collected datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_stats[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_stats` property is a list of datapoints, where each datapoint (captured by a dictionary) specifies the current elapsed timestamp, the score achieved for that timestamp and the hyperparameter configuration for the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the best score achieved per timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_best_scores_from_stats(stats):    \n",
    "    res = []\n",
    "    best_score = float(\"-inf\")\n",
    "    for stat in stats:\n",
    "        best_score = max(best_score, stat[\"score\"])\n",
    "        time_elapsed = stat[\"time_elapsed\"]\n",
    "        res.append({\"best_score\" : best_score, \"time_elapsed\" : time_elapsed})\n",
    "    return res\n",
    "\n",
    "hb_best_score_stats = gather_best_scores_from_stats(hb_stats)\n",
    "rs_best_score_stats = gather_best_scores_from_stats(rs_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our score data, let's compare the performances of random search and hyperband by plotting the best scores they achieve across time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_score_vs_time(rs_data, hb_data, logscale=False):\n",
    "    plt.cla()\n",
    "    \n",
    "    if logscale:\n",
    "        plt.xscale(\"log\")\n",
    "    \n",
    "    xs_hyperband = [x[\"time_elapsed\"] for x in hb_data]\n",
    "    ys_hyperband = [x[\"best_score\"] for x in hb_data]\n",
    "    xs_rs = [x[\"time_elapsed\"] for x in rs_data]\n",
    "    ys_rs = [x[\"best_score\"] for x in rs_data]\n",
    "    \n",
    "    plt.plot(xs_hyperband, ys_hyperband, label=\"Hyperband\")\n",
    "    plt.plot(xs_rs, ys_rs, label=\"RandomSearch\")\n",
    "    \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"F1 Score Achieved\")\n",
    "        \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "plot_score_vs_time(rs_best_score_stats, hb_best_score_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Our plot shows that Hyperband finds a good solution faster than random search. \n",
    "\n",
    "From the plot above, we see that Hyperband finds its best solution at around time 10s, whereas it takes random search around 50-75s to achieve a comparable score.  The relative performance of these algorithms does depend on the specific problem at hand, but the qualitative trends observed here should transfer to many applications in practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that wraps up the hyperparameter tuning tutorial! We hope that MeTaL's hyperparameter tuners are useful for your applications and make hyperparameter tuning enjoyable. Happy tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
