{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Supervision Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we demonstrate how to use the multi-task versions of the label model and end model. We do this with a simple synthetic dataset, focusing primarily on input/output interfaces of these models. In a future tutorial, we will demonstrate the multi-task workflow on a real-world problem with additional scale and complexity, and illustrate the benefits that come from jointly modeling the weak supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-task problems, we execute our pipeline in five steps; for more detail see our latest working [technical draft](https://ajratner.github.io/assets/papers/mts-draft.pdf):\n",
    "1. **Load Data:** As in the `Basics` tutorial, we only have access to unlabeled data points `X`, and noisy labels---which are now in the form of `t` matrices, one for each different _task_.\n",
    "2. **Define Task Graph:** The `TaskGraph` defines the structure of logical relationships between tasks.\n",
    "3. **Train Label Model:** The purpose of the `LabelModel` is again to estimate the unknown accuracies of the labeling functions, _without access to `Y`_, and then use this to denoise and combine them into a set of _probabilistic multi-task training labels_.\n",
    "3. **Train End Model:** We can then use these training labels to supervise a multi-task learning (MTL) model, which optionally inherits network structure from the `TaskGraph`.\n",
    "4. **Evaluate:** We evaluate this model on a held-out test set/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load our data.\n",
    "\n",
    "The data dyptes for the multi-task setting mirror those of the single-task setting, but with an extra dimension for the number of tasks (t), and with the single-task cardinality (k) being replaced by multiple task-specific cardinalities (K_t):\n",
    "\n",
    "* X: a t-length list of \\[n\\]-dim iterables of end model inputs OR a single \\[n\\]-dim iterable of inputs if all tasks operate on the same input\n",
    "* Y: a t-length list of \\[n\\]-dim numpy.ndarray of target labels (Y[i] $\\in$ {1,...,K_t})\n",
    "* L: a t-length list of \\[n,m\\] scipy.sparse matrices of noisy labels (L[i,j] $\\in$ {0,...,K_t}, with label 0 reserved for abstentions\n",
    "\n",
    "And optionally (for use with some debugging/analysis tools):\n",
    "* D: a t-length list of \\[n\\]-dim iterables of human-readable examples (e.g. sentences) OR a single \\[n\\]-dim iterable of examples if all tasks operate on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data that has been pre-split into train/dev/test splits in 80/10/10 proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/multitask_tutorial.pkl\", 'rb') as f:\n",
    "    Xs, Ys, Ls, Ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Task Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary role of the task graph is to define a set of feasible target label vectors.\n",
    "For example, consider the following set of classification tasks, wherein we assign text entities to one of the given labels:\n",
    "\n",
    "T0: Y0 ∈ {PERSON, ORG}  \n",
    "T1: Y1 ∈ {DOCTOR, OTHER PERSON, NOT APPLICABLE}  \n",
    "T2: Y2 ∈ {HOSPITAL, OTHER ORG, NOT APPLICABLE}  \n",
    "\n",
    "Observe that the tasks are related by logical implication relationships: if Y0 = PERSON,\n",
    "then Y2 = NOT APPLICABLE, since Y2 classifies ORGs. Thus, in this task structure, [PERSON, DOCTOR, NOT APPLICABLE] is a feasible label vector, whereas [PERSON, DOCTOR, HOSPITAL] is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reflect this feasible label set, we define our task graph for this problem with a TaskHierarchy, a subclass of TaskGraph which assumes that label K_t for each non-root node is the \"NOT APPLICABLE\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.multitask import TaskHierarchy\n",
    "task_graph = TaskHierarchy(cardinalities=[2,3,3], edges=[(0,1), (0,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass our TaskGraph into the multi-task label model to instantiate a model with the appropriate structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.multitask import MTLabelModel\n",
    "label_model = MTLabelModel(task_graph=task_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the model, computing the overlap matrix $O$ and estimating accuracies $\\mu$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epo]: TRAIN:[loss=4.297]\n",
      "[2 epo]: TRAIN:[loss=4.243]\n",
      "[3 epo]: TRAIN:[loss=4.138]\n",
      "[4 epo]: TRAIN:[loss=3.986]\n",
      "[5 epo]: TRAIN:[loss=3.786]\n",
      "[6 epo]: TRAIN:[loss=3.540]\n",
      "[7 epo]: TRAIN:[loss=3.249]\n",
      "[8 epo]: TRAIN:[loss=2.916]\n",
      "[9 epo]: TRAIN:[loss=2.546]\n",
      "[10 epo]: TRAIN:[loss=2.149]\n",
      "[11 epo]: TRAIN:[loss=1.738]\n",
      "[12 epo]: TRAIN:[loss=1.330]\n",
      "[13 epo]: TRAIN:[loss=0.949]\n",
      "[14 epo]: TRAIN:[loss=0.619]\n",
      "[15 epo]: TRAIN:[loss=0.363]\n",
      "[16 epo]: TRAIN:[loss=0.202]\n",
      "[17 epo]: TRAIN:[loss=0.142]\n",
      "[18 epo]: TRAIN:[loss=0.176]\n",
      "[19 epo]: TRAIN:[loss=0.282]\n",
      "[20 epo]: TRAIN:[loss=0.423]\n",
      "[21 epo]: TRAIN:[loss=0.558]\n",
      "[22 epo]: TRAIN:[loss=0.652]\n",
      "[23 epo]: TRAIN:[loss=0.685]\n",
      "[24 epo]: TRAIN:[loss=0.652]\n",
      "[25 epo]: TRAIN:[loss=0.568]\n",
      "[26 epo]: TRAIN:[loss=0.453]\n",
      "[27 epo]: TRAIN:[loss=0.332]\n",
      "[28 epo]: TRAIN:[loss=0.226]\n",
      "[29 epo]: TRAIN:[loss=0.146]\n",
      "[30 epo]: TRAIN:[loss=0.097]\n",
      "[31 epo]: TRAIN:[loss=0.076]\n",
      "[32 epo]: TRAIN:[loss=0.077]\n",
      "[33 epo]: TRAIN:[loss=0.093]\n",
      "[34 epo]: TRAIN:[loss=0.115]\n",
      "[35 epo]: TRAIN:[loss=0.138]\n",
      "[36 epo]: TRAIN:[loss=0.156]\n",
      "[37 epo]: TRAIN:[loss=0.167]\n",
      "[38 epo]: TRAIN:[loss=0.170]\n",
      "[39 epo]: TRAIN:[loss=0.164]\n",
      "[40 epo]: TRAIN:[loss=0.151]\n",
      "[41 epo]: TRAIN:[loss=0.133]\n",
      "[42 epo]: TRAIN:[loss=0.113]\n",
      "[43 epo]: TRAIN:[loss=0.092]\n",
      "[44 epo]: TRAIN:[loss=0.073]\n",
      "[45 epo]: TRAIN:[loss=0.057]\n",
      "[46 epo]: TRAIN:[loss=0.046]\n",
      "[47 epo]: TRAIN:[loss=0.040]\n",
      "[48 epo]: TRAIN:[loss=0.038]\n",
      "[49 epo]: TRAIN:[loss=0.040]\n",
      "[50 epo]: TRAIN:[loss=0.044]\n",
      "[51 epo]: TRAIN:[loss=0.048]\n",
      "[52 epo]: TRAIN:[loss=0.052]\n",
      "[53 epo]: TRAIN:[loss=0.055]\n",
      "[54 epo]: TRAIN:[loss=0.055]\n",
      "[55 epo]: TRAIN:[loss=0.054]\n",
      "[56 epo]: TRAIN:[loss=0.051]\n",
      "[57 epo]: TRAIN:[loss=0.047]\n",
      "[58 epo]: TRAIN:[loss=0.043]\n",
      "[59 epo]: TRAIN:[loss=0.039]\n",
      "[60 epo]: TRAIN:[loss=0.035]\n",
      "[61 epo]: TRAIN:[loss=0.032]\n",
      "[62 epo]: TRAIN:[loss=0.031]\n",
      "[63 epo]: TRAIN:[loss=0.030]\n",
      "[64 epo]: TRAIN:[loss=0.030]\n",
      "[65 epo]: TRAIN:[loss=0.030]\n",
      "[66 epo]: TRAIN:[loss=0.031]\n",
      "[67 epo]: TRAIN:[loss=0.032]\n",
      "[68 epo]: TRAIN:[loss=0.032]\n",
      "[69 epo]: TRAIN:[loss=0.032]\n",
      "[70 epo]: TRAIN:[loss=0.032]\n",
      "[71 epo]: TRAIN:[loss=0.032]\n",
      "[72 epo]: TRAIN:[loss=0.031]\n",
      "[73 epo]: TRAIN:[loss=0.030]\n",
      "[74 epo]: TRAIN:[loss=0.029]\n",
      "[75 epo]: TRAIN:[loss=0.028]\n",
      "[76 epo]: TRAIN:[loss=0.028]\n",
      "[77 epo]: TRAIN:[loss=0.027]\n",
      "[78 epo]: TRAIN:[loss=0.027]\n",
      "[79 epo]: TRAIN:[loss=0.027]\n",
      "[80 epo]: TRAIN:[loss=0.027]\n",
      "[81 epo]: TRAIN:[loss=0.027]\n",
      "[82 epo]: TRAIN:[loss=0.027]\n",
      "[83 epo]: TRAIN:[loss=0.027]\n",
      "[84 epo]: TRAIN:[loss=0.027]\n",
      "[85 epo]: TRAIN:[loss=0.027]\n",
      "[86 epo]: TRAIN:[loss=0.027]\n",
      "[87 epo]: TRAIN:[loss=0.027]\n",
      "[88 epo]: TRAIN:[loss=0.026]\n",
      "[89 epo]: TRAIN:[loss=0.026]\n",
      "[90 epo]: TRAIN:[loss=0.026]\n",
      "[91 epo]: TRAIN:[loss=0.026]\n",
      "[92 epo]: TRAIN:[loss=0.026]\n",
      "[93 epo]: TRAIN:[loss=0.026]\n",
      "[94 epo]: TRAIN:[loss=0.026]\n",
      "[95 epo]: TRAIN:[loss=0.026]\n",
      "[96 epo]: TRAIN:[loss=0.026]\n",
      "[97 epo]: TRAIN:[loss=0.026]\n",
      "[98 epo]: TRAIN:[loss=0.026]\n",
      "[99 epo]: TRAIN:[loss=0.026]\n",
      "[100 epo]: TRAIN:[loss=0.026]\n",
      "[101 epo]: TRAIN:[loss=0.025]\n",
      "[102 epo]: TRAIN:[loss=0.025]\n",
      "[103 epo]: TRAIN:[loss=0.025]\n",
      "[104 epo]: TRAIN:[loss=0.025]\n",
      "[105 epo]: TRAIN:[loss=0.025]\n",
      "[106 epo]: TRAIN:[loss=0.025]\n",
      "[107 epo]: TRAIN:[loss=0.025]\n",
      "[108 epo]: TRAIN:[loss=0.025]\n",
      "[109 epo]: TRAIN:[loss=0.025]\n",
      "[110 epo]: TRAIN:[loss=0.025]\n",
      "[111 epo]: TRAIN:[loss=0.025]\n",
      "[112 epo]: TRAIN:[loss=0.025]\n",
      "[113 epo]: TRAIN:[loss=0.025]\n",
      "[114 epo]: TRAIN:[loss=0.025]\n",
      "[115 epo]: TRAIN:[loss=0.025]\n",
      "[116 epo]: TRAIN:[loss=0.025]\n",
      "[117 epo]: TRAIN:[loss=0.025]\n",
      "[118 epo]: TRAIN:[loss=0.025]\n",
      "[119 epo]: TRAIN:[loss=0.025]\n",
      "[120 epo]: TRAIN:[loss=0.025]\n",
      "[121 epo]: TRAIN:[loss=0.025]\n",
      "[122 epo]: TRAIN:[loss=0.025]\n",
      "[123 epo]: TRAIN:[loss=0.025]\n",
      "[124 epo]: TRAIN:[loss=0.025]\n",
      "[125 epo]: TRAIN:[loss=0.025]\n",
      "[126 epo]: TRAIN:[loss=0.025]\n",
      "[127 epo]: TRAIN:[loss=0.025]\n",
      "[128 epo]: TRAIN:[loss=0.025]\n",
      "[129 epo]: TRAIN:[loss=0.025]\n",
      "[130 epo]: TRAIN:[loss=0.025]\n",
      "[131 epo]: TRAIN:[loss=0.025]\n",
      "[132 epo]: TRAIN:[loss=0.025]\n",
      "[133 epo]: TRAIN:[loss=0.025]\n",
      "[134 epo]: TRAIN:[loss=0.025]\n",
      "[135 epo]: TRAIN:[loss=0.025]\n",
      "[136 epo]: TRAIN:[loss=0.025]\n",
      "[137 epo]: TRAIN:[loss=0.025]\n",
      "[138 epo]: TRAIN:[loss=0.025]\n",
      "[139 epo]: TRAIN:[loss=0.025]\n",
      "[140 epo]: TRAIN:[loss=0.025]\n",
      "[141 epo]: TRAIN:[loss=0.025]\n",
      "[142 epo]: TRAIN:[loss=0.025]\n",
      "[143 epo]: TRAIN:[loss=0.025]\n",
      "[144 epo]: TRAIN:[loss=0.025]\n",
      "[145 epo]: TRAIN:[loss=0.025]\n",
      "[146 epo]: TRAIN:[loss=0.025]\n",
      "[147 epo]: TRAIN:[loss=0.025]\n",
      "[148 epo]: TRAIN:[loss=0.025]\n",
      "[149 epo]: TRAIN:[loss=0.025]\n",
      "[150 epo]: TRAIN:[loss=0.025]\n",
      "[151 epo]: TRAIN:[loss=0.025]\n",
      "[152 epo]: TRAIN:[loss=0.025]\n",
      "[153 epo]: TRAIN:[loss=0.025]\n",
      "[154 epo]: TRAIN:[loss=0.025]\n",
      "[155 epo]: TRAIN:[loss=0.025]\n",
      "[156 epo]: TRAIN:[loss=0.025]\n",
      "[157 epo]: TRAIN:[loss=0.025]\n",
      "[158 epo]: TRAIN:[loss=0.025]\n",
      "[159 epo]: TRAIN:[loss=0.025]\n",
      "[160 epo]: TRAIN:[loss=0.025]\n",
      "[161 epo]: TRAIN:[loss=0.025]\n",
      "[162 epo]: TRAIN:[loss=0.025]\n",
      "[163 epo]: TRAIN:[loss=0.025]\n",
      "[164 epo]: TRAIN:[loss=0.025]\n",
      "[165 epo]: TRAIN:[loss=0.025]\n",
      "[166 epo]: TRAIN:[loss=0.025]\n",
      "[167 epo]: TRAIN:[loss=0.025]\n",
      "[168 epo]: TRAIN:[loss=0.025]\n",
      "[169 epo]: TRAIN:[loss=0.025]\n",
      "[170 epo]: TRAIN:[loss=0.025]\n",
      "[171 epo]: TRAIN:[loss=0.025]\n",
      "[172 epo]: TRAIN:[loss=0.025]\n",
      "[173 epo]: TRAIN:[loss=0.025]\n",
      "[174 epo]: TRAIN:[loss=0.025]\n",
      "[175 epo]: TRAIN:[loss=0.025]\n",
      "[176 epo]: TRAIN:[loss=0.025]\n",
      "[177 epo]: TRAIN:[loss=0.025]\n",
      "[178 epo]: TRAIN:[loss=0.025]\n",
      "[179 epo]: TRAIN:[loss=0.025]\n",
      "[180 epo]: TRAIN:[loss=0.025]\n",
      "[181 epo]: TRAIN:[loss=0.025]\n",
      "[182 epo]: TRAIN:[loss=0.025]\n",
      "[183 epo]: TRAIN:[loss=0.025]\n",
      "[184 epo]: TRAIN:[loss=0.025]\n",
      "[185 epo]: TRAIN:[loss=0.025]\n",
      "[186 epo]: TRAIN:[loss=0.025]\n",
      "[187 epo]: TRAIN:[loss=0.025]\n",
      "[188 epo]: TRAIN:[loss=0.025]\n",
      "[189 epo]: TRAIN:[loss=0.025]\n",
      "[190 epo]: TRAIN:[loss=0.025]\n",
      "[191 epo]: TRAIN:[loss=0.025]\n",
      "[192 epo]: TRAIN:[loss=0.025]\n",
      "[193 epo]: TRAIN:[loss=0.025]\n",
      "[194 epo]: TRAIN:[loss=0.025]\n",
      "[195 epo]: TRAIN:[loss=0.025]\n",
      "[196 epo]: TRAIN:[loss=0.025]\n",
      "[197 epo]: TRAIN:[loss=0.025]\n",
      "[198 epo]: TRAIN:[loss=0.025]\n",
      "[199 epo]: TRAIN:[loss=0.025]\n",
      "[200 epo]: TRAIN:[loss=0.025]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "label_model.train_model(Ls[0], n_epochs=200, print_every=20, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<100x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 846 stored elements in Compressed Sparse Row format>,\n",
       " <100x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 846 stored elements in Compressed Sparse Row format>,\n",
       " <100x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 846 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ls[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the single-task case, we can score this trained model to evaluate it directly, or use it to make predictions for our training set that will then be used to train a multi-task end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_model.score((Ls[1], Ys[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_ps stands for \"Y[labels]_train[split]_p[redicted]s[oft]\"\n",
    "Y_train_ps = label_model.predict_proba(Ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the single-task end model, the multi-task end model consists of three components: input layers, middle layers, and task head layers. Again, each layer consists of a torch.nn.Module followed by various optional additional operators (e.g., a ReLU nonlinearity, batch normalization, and/or dropout).\n",
    "\n",
    "**Input layers**: The input module is an IdentityModule by default. If your tasks accept inputs of different types (e.g., one task over images and another over text), you may pass in a t-length list of input modules.\n",
    "\n",
    "**Middle layers**: The middle modules are nn.Linear by default and are shared by all tasks.\n",
    "\n",
    "**Head layers**: The t task head modules are nn.Linear modules by default. You may instead pass in a custom module to be used by all tasks or a t-length list of modules. These task heads are unique to each task, sharing no parameters with other tasks. Their output is fed to a set of softmax operators whose output dimensions are equal to the cardinalities for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a simple graph with a single (identity) input module, two intermediate layers, and linear task heads attached to the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "\n",
      "--Input Layer--\n",
      "IdentityModule()\n",
      "\n",
      "--Middle Layers--\n",
      "(layer1):\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "(layer2):\n",
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "(head0)\n",
      "Linear(in_features=10, out_features=2, bias=True)\n",
      "(head1)\n",
      "Linear(in_features=10, out_features=3, bias=True)\n",
      "(head2)\n",
      "Linear(in_features=10, out_features=3, bias=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metal.multitask import MTEndModel\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "end_model = MTEndModel([1000,100,10], task_graph=task_graph, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 epo]: TRAIN:[loss=2.197] VALID:[accuracy=0.790]\n",
      "Saving model at iteration 1 with best score 0.790\n",
      "[2 epo]: TRAIN:[loss=1.370] VALID:[accuracy=0.903]\n",
      "Saving model at iteration 2 with best score 0.903\n",
      "[3 epo]: TRAIN:[loss=1.032] VALID:[accuracy=0.900]\n",
      "[4 epo]: TRAIN:[loss=0.896] VALID:[accuracy=0.900]\n",
      "[5 epo]: TRAIN:[loss=0.857] VALID:[accuracy=0.877]\n",
      "Restoring best model from iteration 2 with score 0.903\n",
      "Finished Training\n",
      "Accuracy: 0.903\n"
     ]
    }
   ],
   "source": [
    "end_model.train_model((Xs[0], Y_train_ps), valid_data=(Xs[1], Ys[1]), n_epochs=5, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes scoring our multi-task models, by default the mean task accuracy is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model:\n",
      "Accuracy: 0.857\n",
      "\n",
      "End Model:\n",
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "print(\"Label Model:\")\n",
    "score = label_model.score((Ls[2], Ys[2]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"End Model:\")\n",
    "score = end_model.score((Xs[2], Ys[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also, however, pass `reduce=None` to get back a list of task-specific accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (t=0): 0.920\n",
      "Accuracy (t=1): 0.900\n",
      "Accuracy (t=2): 0.910\n"
     ]
    }
   ],
   "source": [
    "scores = end_model.score((Xs[2], Ys[2]), reduce=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the predictions for all three tasks, we can call predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1,\n",
       "        2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
       "        1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "        2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1]),\n",
       " array([3, 2, 1, 3, 2, 1, 2, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1,\n",
       "        3, 3, 3, 3, 2, 3, 1, 1, 2, 3, 1, 3, 3, 2, 2, 3, 2, 3, 3, 1, 1, 1,\n",
       "        2, 1, 1, 2, 1, 3, 2, 2, 2, 2, 3, 2, 3, 1, 1, 3, 1, 1, 3, 3, 1, 3,\n",
       "        1, 3, 2, 2, 3, 3, 1, 3, 2, 3, 3, 2, 1, 3, 1, 2, 1, 3, 3, 3, 3, 3,\n",
       "        3, 3, 2, 1, 1, 1, 1, 3, 3, 3, 2, 2]),\n",
       " array([1, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 1, 2, 1, 2, 2, 1, 1, 3, 2, 3, 3,\n",
       "        2, 1, 2, 2, 3, 2, 3, 3, 3, 2, 3, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 1, 1, 3, 1,\n",
       "        3, 1, 3, 3, 2, 2, 3, 2, 3, 2, 2, 3, 3, 1, 3, 3, 3, 1, 2, 1, 2, 1,\n",
       "        2, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 3])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_p = end_model.predict(Xs[2])\n",
    "Y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
