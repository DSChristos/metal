{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate the true class balance to be recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56401127, 0.29391442, 0.1420743 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "# Generate the true class balance to be recovered\n",
    "p_Y = np.random.random(K)\n",
    "p_Y /= p_Y.sum()\n",
    "p_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate the true conditional probability tables (CPTs) for the LFs\n",
    "\n",
    "Separate simple process here to keep simple (later merge this with the SPA generator).\n",
    "Generate in terms of the _conditional accuracies_ (which is equivalent to the recall...):\n",
    "$$\n",
    "\\alpha_{i,y',y} = P(\\lambda_i = y' | Y = y)\n",
    "$$\n",
    "\n",
    "Note that this table should be normalized such that:\n",
    "$$\n",
    "\\sum_{y'} \\alpha_{i,y',y} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4754549 , 0.27264056, 0.26943463],\n",
       "       [0.06054756, 0.18850857, 0.28989351],\n",
       "       [0.46399755, 0.53885088, 0.44067186]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 10\n",
    "alphas = []\n",
    "for i in range(M):\n",
    "    a = np.random.random((K,K))\n",
    "    alphas.append( a @ np.diag(1 / a.sum(axis=0)) )\n",
    "alpha = np.array(alphas)\n",
    "\n",
    "assert np.all(np.abs(alpha.sum(axis=1) - 1) < 1e-5)\n",
    "alpha[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Different tensor product approaches in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 1.92 ms, total: 118 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        O_1[i,j,k,y1,y2,y3] += alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 ms, sys: 549 µs, total: 1.94 ms\n",
      "Wall time: 1.17 ms\n"
     ]
    }
   ],
   "source": [
    "%time O_3 = np.einsum('aby,cdy,efy->acebdf', alpha, alpha, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.962686660289687e-18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(O_1 - O_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying a bilinear form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 507 ms, sys: 8.58 ms, total: 516 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Op_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        Op_1[i,j,k,y1,y2,y3] += p_Y[y] * alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 736 µs, sys: 138 µs, total: 874 µs\n",
      "Wall time: 533 µs\n"
     ]
    }
   ],
   "source": [
    "%time Op_3 = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.043199071866942e-18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(Op_1 - Op_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate the _three-way_ overlaps tensor $O$ of conditionally-independent LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly generate $O$.\n",
    "By our conditional independence assumption, we have:\n",
    "$$\n",
    "P(\\lambda_i = y', \\lambda_j = y'' | Y = y) = \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "O_{i,j,y',y''} = \\sum_y P(Y=y) \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mask\n",
    "mask = torch.ones((M,M,M,K,K,K)).byte()\n",
    "for i, j, k in product(range(M), repeat=3):\n",
    "    if len(set((i,j,k))) < 3:\n",
    "        mask[i,j,k,:,:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 927 µs, total: 13.2 ms\n",
      "Wall time: 3.18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)\n",
    "O = torch.from_numpy(O).float()\n",
    "O[1-mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise labeling rates\n",
    "mask_2 = torch.ones((M,M,K,K)).byte()\n",
    "for i in range(M):\n",
    "    mask_2[i,i,:,:] = 0\n",
    "\n",
    "O_2 = np.einsum('aby,cdy,y->acbd', alpha, alpha, p_Y)\n",
    "O_2 = torch.from_numpy(O_2).float()\n",
    "O_2[1-mask_2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed labeling rates\n",
    "O_l = torch.from_numpy(np.einsum('aby,y->ab', alpha, p_Y)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Try to recover $\\alpha$ given $P(Y=y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(A, P, O, O_l, O_2):\n",
    "    \n",
    "    # Main constraint: match empirical three-way overlaps matrix (entries O_ijk for i != j != k)\n",
    "    loss_1 = torch.norm((O - torch.einsum('aby,cdy,efy,y->acebdf', [A,A,A,P]))[mask])**2\n",
    "    \n",
    "    # Col-wise stochastic: \\sum_y' P(\\lf=y'|Y=y) = 1.0\n",
    "    loss_2 = torch.norm(torch.sum(A, 1) - 1)**2\n",
    "    \n",
    "    # Row-wise constraint: match observed labeling rates P(\\lf=y') = \\sum_y P(Y=y) P(\\lf=y'|Y=y)\n",
    "    loss_3 = torch.norm(O_l - torch.einsum('aby,y->ab', [A,P]))**2\n",
    "    \n",
    "    # Pairwise observed: match empirical pairwise overlaps matrix (entries O_ij for i != j)\n",
    "    # loss_4 = torch.norm((O_2 - torch.einsum('aby,cdy,y->acbd', [A,A,P]))[mask_2])**2\n",
    "    \n",
    "    return loss_1 + loss_2 + loss_3 # + loss_4\n",
    "\n",
    "def train_model(A, P, O, O_l, O_2, n_epochs=10, lr=0.01, momentum=0, print_every=1):\n",
    "    optimizer = optim.SGD([A], lr=lr, momentum=momentum)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to calculate outputs\n",
    "        loss = get_loss(A, P, O, O_l, O_2)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep running sum of losses\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "        # Report progress\n",
    "        if epoch % print_every == 0:\n",
    "            msg = f\"[E:{epoch}]\\tLoss: {epoch_loss:.8f}\"\n",
    "            print(msg)\n",
    "\n",
    "def train_model_lbfgs(A, P, O, O_l, O_2, n_epochs=10, lr=1, print_every=1):\n",
    "    optimizer = optim.LBFGS([A], lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):        \n",
    "        def closure():\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to calculate outputs\n",
    "            loss = get_loss(A, P, O, O_l, O_2)\n",
    "\n",
    "            # Backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Report progress\n",
    "            if epoch % print_every == 0:\n",
    "                msg = f\"[E:{epoch}]\\tLoss: {loss.detach():.8f}\"\n",
    "                print(msg)\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        # Perform optimizer step\n",
    "        optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E:0]\tLoss: 425.36105347\n",
      "[E:0]\tLoss: 340.80267334\n",
      "[E:0]\tLoss: 109.66790771\n",
      "[E:0]\tLoss: 53.99650574\n",
      "[E:0]\tLoss: 26.19582939\n",
      "[E:0]\tLoss: 14.41246223\n",
      "[E:0]\tLoss: 6.41153574\n",
      "[E:0]\tLoss: 2.53658915\n",
      "[E:0]\tLoss: 1.73112893\n",
      "[E:0]\tLoss: 1.07614517\n",
      "[E:0]\tLoss: 0.84402746\n",
      "[E:0]\tLoss: 0.68905640\n",
      "[E:0]\tLoss: 0.50893801\n",
      "[E:0]\tLoss: 0.35955188\n",
      "[E:0]\tLoss: 0.22685249\n",
      "[E:0]\tLoss: 0.20317608\n",
      "[E:0]\tLoss: 0.19488670\n",
      "[E:0]\tLoss: 0.18258806\n",
      "[E:0]\tLoss: 0.16458936\n",
      "[E:0]\tLoss: 0.16071397\n",
      "[E:1]\tLoss: 0.14896670\n",
      "[E:1]\tLoss: 0.14194909\n",
      "[E:1]\tLoss: 0.12217625\n",
      "[E:1]\tLoss: 0.09708909\n",
      "[E:1]\tLoss: 0.06923153\n",
      "[E:1]\tLoss: 0.03416957\n",
      "[E:1]\tLoss: 0.03278351\n",
      "[E:1]\tLoss: 0.02825810\n",
      "[E:1]\tLoss: 0.02700087\n",
      "[E:1]\tLoss: 0.02570152\n",
      "[E:1]\tLoss: 0.02235990\n",
      "[E:1]\tLoss: 0.02113901\n",
      "[E:1]\tLoss: 0.01847202\n",
      "[E:1]\tLoss: 0.01502717\n",
      "[E:1]\tLoss: 0.01050317\n",
      "[E:1]\tLoss: 0.00834673\n",
      "[E:1]\tLoss: 0.00520078\n",
      "[E:1]\tLoss: 0.00453616\n",
      "[E:1]\tLoss: 0.00375597\n",
      "[E:1]\tLoss: 0.00274836\n",
      "[E:2]\tLoss: 0.00172170\n",
      "[E:2]\tLoss: 0.00121723\n",
      "[E:2]\tLoss: 0.00072347\n",
      "[E:2]\tLoss: 0.00043284\n",
      "[E:2]\tLoss: 0.00017910\n",
      "[E:2]\tLoss: 0.00010100\n",
      "[E:2]\tLoss: 0.00007789\n",
      "[E:2]\tLoss: 0.00005332\n",
      "[E:2]\tLoss: 0.00003163\n",
      "[E:2]\tLoss: 0.00001930\n",
      "[E:2]\tLoss: 0.00001138\n",
      "[E:2]\tLoss: 0.00000740\n",
      "[E:2]\tLoss: 0.00000384\n",
      "[E:2]\tLoss: 0.00000337\n",
      "[E:2]\tLoss: 0.00000120\n",
      "[E:2]\tLoss: 0.00000097\n",
      "[E:2]\tLoss: 0.00000071\n",
      "[E:2]\tLoss: 0.00000049\n",
      "[E:2]\tLoss: 0.00000032\n",
      "[E:2]\tLoss: 0.00000023\n",
      "[E:3]\tLoss: 0.00000016\n",
      "[E:3]\tLoss: 0.00000009\n",
      "[E:3]\tLoss: 0.00000004\n",
      "[E:3]\tLoss: 0.00000002\n",
      "[E:3]\tLoss: 0.00000001\n",
      "[E:3]\tLoss: 0.00000001\n",
      "[E:3]\tLoss: 0.00000001\n",
      "[E:3]\tLoss: 0.00000001\n",
      "[E:3]\tLoss: 0.00000000\n",
      "[E:3]\tLoss: 0.00000000\n",
      "[E:3]\tLoss: 0.00000000\n",
      "[E:4]\tLoss: 0.00000000\n",
      "[E:4]\tLoss: 0.00000000\n",
      "[E:5]\tLoss: 0.00000000\n",
      "[E:5]\tLoss: 0.00000000\n",
      "[E:6]\tLoss: 0.00000000\n",
      "[E:6]\tLoss: 0.00000000\n",
      "[E:7]\tLoss: 0.00000000\n",
      "[E:7]\tLoss: 0.00000000\n",
      "[E:8]\tLoss: 0.00000000\n",
      "[E:8]\tLoss: 0.00000000\n",
      "[E:9]\tLoss: 0.00000000\n",
      "[E:9]\tLoss: 0.00000000\n",
      "Param estimation error: 1.1787149367606955e-06\n"
     ]
    }
   ],
   "source": [
    "A = nn.Parameter(torch.from_numpy(np.random.rand(M, K, K)).float()).float()\n",
    "P = torch.from_numpy(p_Y).float()\n",
    "\n",
    "# train_model(A, P, O, O_l, O_2, n_epochs=10000, lr=0.005, momentum=0.9, print_every=1000)\n",
    "train_model_lbfgs(A, P, O, O_l, O_2, n_epochs=10, print_every=1)\n",
    "\n",
    "print(f\"Param estimation error: {np.mean(np.abs(A.detach().numpy() - alpha))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4755, 0.2726, 0.2694],\n",
       "        [0.0605, 0.1885, 0.2899],\n",
       "        [0.4640, 0.5388, 0.4407]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4754549 , 0.27264056, 0.26943463],\n",
       "       [0.06054756, 0.18850857, 0.28989351],\n",
       "       [0.46399755, 0.53885088, 0.44067186]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0175, 0.5390, 1.4435], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01753008, 0.53894963, 1.44352029])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (MeTaL)",
   "language": "python",
   "name": "metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
