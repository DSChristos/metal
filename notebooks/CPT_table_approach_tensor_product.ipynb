{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate the true class balance to be recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33627531, 0.06528622, 0.59843847])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "# Generate the true class balance to be recovered\n",
    "p_Y = np.random.random(K)\n",
    "p_Y /= p_Y.sum()\n",
    "p_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate the true conditional probability tables (CPTs) for the LFs\n",
    "\n",
    "Separate simple process here to keep simple (later merge this with the SPA generator).\n",
    "Generate in terms of the _conditional accuracies_ (which is equivalent to the recall...):\n",
    "$$\n",
    "\\alpha_{i,y',y} = P(\\lambda_i = y' | Y = y)\n",
    "$$\n",
    "\n",
    "Note that this table should be normalized such that:\n",
    "$$\n",
    "\\sum_{y'} \\alpha_{i,y',y} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28359862, 0.23194285, 0.39931097],\n",
       "       [0.36570757, 0.39197016, 0.31187131],\n",
       "       [0.35069381, 0.376087  , 0.28881771]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 10\n",
    "alphas = []\n",
    "for i in range(M):\n",
    "    a = np.random.random((K,K))\n",
    "    alphas.append( a @ np.diag(1 / a.sum(axis=0)) )\n",
    "alpha = np.array(alphas)\n",
    "\n",
    "assert np.all(np.abs(alpha.sum(axis=1) - 1) < 1e-5)\n",
    "alpha[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Different tensor product approaches in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 ms, sys: 817 µs, total: 130 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        O_1[i,j,k,y1,y2,y3] += alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 ms, sys: 496 µs, total: 1.94 ms\n",
      "Wall time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time O_3 = np.einsum('aby,cdy,efy->acebdf', alpha, alpha, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.096404928229565e-18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(O_1 - O_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying a bilinear form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 495 ms, sys: 4.54 ms, total: 500 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Op_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        Op_1[i,j,k,y1,y2,y3] += p_Y[y] * alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 622 µs, sys: 118 µs, total: 740 µs\n",
      "Wall time: 530 µs\n"
     ]
    }
   ],
   "source": [
    "%time Op_3 = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1384522560651964e-18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(Op_1 - Op_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate the _three-way_ overlaps tensor $O$ of conditionally-independent LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly generate $O$.\n",
    "By our conditional independence assumption, we have:\n",
    "$$\n",
    "P(\\lambda_i = y', \\lambda_j = y'' | Y = y) = \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "O_{i,j,y',y''} = \\sum_y P(Y=y) \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mask\n",
    "mask = torch.ones((M,M,M,K,K,K)).byte()\n",
    "for i, j, k in product(range(M), repeat=3):\n",
    "    if len(set((i,j,k))) < 3:\n",
    "        mask[i,j,k,:,:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 ms, sys: 980 µs, total: 13.1 ms\n",
      "Wall time: 3.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)\n",
    "O = torch.from_numpy(O).float()\n",
    "O[1-mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise labeling rates\n",
    "mask_2 = torch.ones((M,M,K,K)).byte()\n",
    "for i in range(M):\n",
    "    mask_2[i,i,:,:] = 0\n",
    "\n",
    "O_2 = np.einsum('aby,cdy,y->acbd', alpha, alpha, p_Y)\n",
    "O_2 = torch.from_numpy(O_2).float()\n",
    "O_2[1-mask_2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed labeling rates\n",
    "O_l = torch.from_numpy(np.einsum('aby,y->ab', alpha, p_Y)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Try to recover $\\alpha$ given $P(Y=y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(A, P, O, O_l, O_2):\n",
    "    \n",
    "    # Main constraint: match empirical three-way overlaps matrix (entries O_ijk for i != j != k)\n",
    "    loss_1 = torch.norm((O - torch.einsum('aby,cdy,efy,y->acebdf', [A,A,A,P]))[mask])**2\n",
    "    \n",
    "    # Col-wise stochastic: \\sum_y' P(\\lf=y'|Y=y) = 1.0\n",
    "    loss_2 = torch.norm(torch.sum(A, 1) - 1)**2\n",
    "    \n",
    "    # Row-wise constraint: match observed labeling rates P(\\lf=y') = \\sum_y P(Y=y) P(\\lf=y'|Y=y)\n",
    "    loss_3 = torch.norm(O_l - torch.einsum('aby,y->ab', [A,P]))**2\n",
    "    \n",
    "    # Pairwise observed: match empirical pairwise overlaps matrix (entries O_ij for i != j)\n",
    "    # loss_4 = torch.norm((O_2 - torch.einsum('aby,cdy,y->acbd', [A,A,P]))[mask_2])**2\n",
    "    \n",
    "    return loss_1 + loss_2 + loss_3 # + loss_4\n",
    "\n",
    "def train_model(A, P, O, O_l, O_2, n_epochs=10, lr=0.01, momentum=0, print_every=1):\n",
    "    optimizer = optim.SGD([A], lr=lr, momentum=momentum)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to calculate outputs\n",
    "        loss = get_loss(A, P, O, O_l, O_2)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep running sum of losses\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "        # Report progress\n",
    "        if epoch % print_every == 0:\n",
    "            msg = f\"[E:{epoch}]\\tLoss: {epoch_loss:.8f}\"\n",
    "            print(msg)\n",
    "\n",
    "def train_model_lbfgs(A, P, O, O_l, O_2, n_epochs=10, lr=1, print_every=1):\n",
    "    optimizer = optim.LBFGS([A], lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):        \n",
    "        def closure():\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to calculate outputs\n",
    "            loss = get_loss(A, P, O, O_l, O_2)\n",
    "\n",
    "            # Backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Report progress\n",
    "            if epoch % print_every == 0:\n",
    "                msg = f\"[E:{epoch}]\\tLoss: {loss.detach():.8f}\"\n",
    "                print(msg)\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        # Perform optimizer step\n",
    "        optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E:0]\tLoss: 177.65818787\n",
      "[E:0]\tLoss: 144.57876587\n",
      "[E:0]\tLoss: 58.89773178\n",
      "[E:0]\tLoss: 35.49965668\n",
      "[E:0]\tLoss: 18.02413940\n",
      "[E:0]\tLoss: 4.31744003\n",
      "[E:0]\tLoss: 2.35179520\n",
      "[E:0]\tLoss: 1.00241983\n",
      "[E:0]\tLoss: 0.57635510\n",
      "[E:0]\tLoss: 0.44058844\n",
      "[E:0]\tLoss: 0.41010287\n",
      "[E:0]\tLoss: 0.24305074\n",
      "[E:0]\tLoss: 0.17068177\n",
      "[E:0]\tLoss: 0.09264879\n",
      "[E:0]\tLoss: 0.07045437\n",
      "[E:0]\tLoss: 0.06329033\n",
      "[E:0]\tLoss: 0.05925115\n",
      "[E:0]\tLoss: 0.05787576\n",
      "[E:0]\tLoss: 0.05537866\n",
      "[E:0]\tLoss: 0.05238256\n",
      "[E:1]\tLoss: 0.04945649\n",
      "[E:1]\tLoss: 0.04760237\n",
      "[E:1]\tLoss: 0.04589147\n",
      "[E:1]\tLoss: 0.04376334\n",
      "[E:1]\tLoss: 0.03950287\n",
      "[E:1]\tLoss: 0.03583579\n",
      "[E:1]\tLoss: 0.03326323\n",
      "[E:1]\tLoss: 0.02985516\n",
      "[E:1]\tLoss: 0.02592331\n",
      "[E:1]\tLoss: 0.02189494\n",
      "[E:1]\tLoss: 0.01907339\n",
      "[E:1]\tLoss: 0.01803201\n",
      "[E:1]\tLoss: 0.01729955\n",
      "[E:1]\tLoss: 0.01655176\n",
      "[E:1]\tLoss: 0.01559204\n",
      "[E:1]\tLoss: 0.01457048\n",
      "[E:1]\tLoss: 0.01313580\n",
      "[E:1]\tLoss: 0.01159024\n",
      "[E:1]\tLoss: 0.00931841\n",
      "[E:1]\tLoss: 0.00672343\n",
      "[E:2]\tLoss: 0.00397075\n",
      "[E:2]\tLoss: 0.00212902\n",
      "[E:2]\tLoss: 0.00148414\n",
      "[E:2]\tLoss: 0.00102373\n",
      "[E:2]\tLoss: 0.00071573\n",
      "[E:2]\tLoss: 0.00065970\n",
      "[E:2]\tLoss: 0.00063998\n",
      "[E:2]\tLoss: 0.00061812\n",
      "[E:2]\tLoss: 0.00059031\n",
      "[E:2]\tLoss: 0.00054804\n",
      "[E:2]\tLoss: 0.00047589\n",
      "[E:2]\tLoss: 0.00036441\n",
      "[E:2]\tLoss: 0.00026827\n",
      "[E:2]\tLoss: 0.00019305\n",
      "[E:2]\tLoss: 0.00013177\n",
      "[E:2]\tLoss: 0.00009608\n",
      "[E:2]\tLoss: 0.00008010\n",
      "[E:2]\tLoss: 0.00007408\n",
      "[E:2]\tLoss: 0.00007292\n",
      "[E:2]\tLoss: 0.00006999\n",
      "[E:3]\tLoss: 0.00006569\n",
      "[E:3]\tLoss: 0.00006095\n",
      "[E:3]\tLoss: 0.00005593\n",
      "[E:3]\tLoss: 0.00004479\n",
      "[E:3]\tLoss: 0.00003276\n",
      "[E:3]\tLoss: 0.00001898\n",
      "[E:3]\tLoss: 0.00001454\n",
      "[E:3]\tLoss: 0.00001085\n",
      "[E:3]\tLoss: 0.00001067\n",
      "[E:3]\tLoss: 0.00001032\n",
      "[E:3]\tLoss: 0.00001002\n",
      "[E:3]\tLoss: 0.00000975\n",
      "[E:3]\tLoss: 0.00000949\n",
      "[E:3]\tLoss: 0.00000915\n",
      "[E:3]\tLoss: 0.00000802\n",
      "[E:3]\tLoss: 0.00000647\n",
      "[E:3]\tLoss: 0.00000448\n",
      "[E:3]\tLoss: 0.00000270\n",
      "[E:3]\tLoss: 0.00000250\n",
      "[E:3]\tLoss: 0.00000209\n",
      "[E:4]\tLoss: 0.00000199\n",
      "[E:4]\tLoss: 0.00000179\n",
      "[E:4]\tLoss: 0.00000169\n",
      "[E:4]\tLoss: 0.00000166\n",
      "[E:4]\tLoss: 0.00000157\n",
      "[E:4]\tLoss: 0.00000150\n",
      "[E:4]\tLoss: 0.00000133\n",
      "[E:4]\tLoss: 0.00000104\n",
      "[E:4]\tLoss: 0.00000053\n",
      "[E:4]\tLoss: 0.00000266\n",
      "[E:4]\tLoss: 0.00000015\n",
      "[E:4]\tLoss: 0.00000009\n",
      "[E:4]\tLoss: 0.00000007\n",
      "[E:4]\tLoss: 0.00000006\n",
      "[E:4]\tLoss: 0.00000005\n",
      "[E:4]\tLoss: 0.00000005\n",
      "[E:4]\tLoss: 0.00000004\n",
      "[E:4]\tLoss: 0.00000004\n",
      "[E:4]\tLoss: 0.00000003\n",
      "[E:4]\tLoss: 0.00000002\n",
      "[E:5]\tLoss: 0.00000001\n",
      "[E:5]\tLoss: 0.00000007\n",
      "[E:5]\tLoss: 0.00000000\n",
      "[E:5]\tLoss: 0.00000000\n",
      "[E:6]\tLoss: 0.00000000\n",
      "[E:6]\tLoss: 0.00000000\n",
      "[E:7]\tLoss: 0.00000000\n",
      "[E:7]\tLoss: 0.00000000\n",
      "[E:8]\tLoss: 0.00000000\n",
      "[E:8]\tLoss: 0.00000000\n",
      "[E:9]\tLoss: 0.00000000\n",
      "[E:9]\tLoss: 0.00000000\n",
      "Param estimation error: 5.077935413926606e-06\n"
     ]
    }
   ],
   "source": [
    "A = nn.Parameter(torch.from_numpy(np.random.rand(M, K, K)).float()).float()\n",
    "P = torch.from_numpy(p_Y).float()\n",
    "\n",
    "# train_model(A, P, O, O_l, O_2, n_epochs=10000, lr=0.005, momentum=0.9, print_every=1000)\n",
    "train_model_lbfgs(A, P, O, O_l, O_2, n_epochs=10, print_every=1)\n",
    "\n",
    "print(f\"Param estimation error: {np.mean(np.abs(A.detach().numpy() - alpha))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2836, 0.2320, 0.3993],\n",
       "        [0.3657, 0.3920, 0.3119],\n",
       "        [0.3507, 0.3761, 0.2888]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28359862, 0.23194285, 0.39931097],\n",
       "       [0.36570757, 0.39197016, 0.31187131],\n",
       "       [0.35069381, 0.376087  , 0.28881771]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9199, 1.0672, 1.0129], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91485244, 1.06954904, 1.01559852])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (MeTaL)",
   "language": "python",
   "name": "metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
