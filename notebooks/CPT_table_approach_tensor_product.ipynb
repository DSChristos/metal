{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate the true class balance to be recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33627531, 0.06528622, 0.59843847])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "# Generate the true class balance to be recovered\n",
    "p_Y = np.random.random(K)\n",
    "p_Y /= p_Y.sum()\n",
    "p_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate the true conditional probability tables (CPTs) for the LFs\n",
    "\n",
    "Separate simple process here to keep simple (later merge this with the SPA generator).\n",
    "Generate in terms of the _conditional accuracies_ (which is equivalent to the recall...):\n",
    "$$\n",
    "\\alpha_{i,y',y} = P(\\lambda_i = y' | Y = y)\n",
    "$$\n",
    "\n",
    "Note that this table should be normalized such that:\n",
    "$$\n",
    "\\sum_{y'} \\alpha_{i,y',y} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28359862, 0.23194285, 0.39931097],\n",
       "       [0.36570757, 0.39197016, 0.31187131],\n",
       "       [0.35069381, 0.376087  , 0.28881771]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 10\n",
    "alphas = []\n",
    "for i in range(M):\n",
    "    a = np.random.random((K,K))\n",
    "    alphas.append( a @ np.diag(1 / a.sum(axis=0)) )\n",
    "alpha = np.array(alphas)\n",
    "\n",
    "assert np.all(np.abs(alpha.sum(axis=1) - 1) < 1e-5)\n",
    "alpha[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Different tensor product approaches in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 ms, sys: 817 µs, total: 130 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        O_1[i,j,k,y1,y2,y3] += alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 ms, sys: 496 µs, total: 1.94 ms\n",
      "Wall time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time O_3 = np.einsum('aby,cdy,efy->acebdf', alpha, alpha, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.096404928229565e-18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(O_1 - O_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying a bilinear form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 495 ms, sys: 4.54 ms, total: 500 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Op_1 = np.zeros((M,M,M,K,K,K))\n",
    "for i, j, k, y1, y2, y3 in product(range(M), range(M), range(M), range(K), range(K), range(K)):\n",
    "    for y in range(K):\n",
    "        Op_1[i,j,k,y1,y2,y3] += p_Y[y] * alpha[i, y1, y] * alpha[j, y2, y] * alpha[k, y3, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 622 µs, sys: 118 µs, total: 740 µs\n",
      "Wall time: 530 µs\n"
     ]
    }
   ],
   "source": [
    "%time Op_3 = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1384522560651964e-18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(Op_1 - Op_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate the _three-way_ overlaps tensor $O$ of conditionally-independent LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly generate $O$.\n",
    "By our conditional independence assumption, we have:\n",
    "$$\n",
    "P(\\lambda_i = y', \\lambda_j = y'' | Y = y) = \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "O_{i,j,y',y''} = \\sum_y P(Y=y) \\alpha_{i,y',y} \\alpha_{j,y'',y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mask\n",
    "mask = torch.ones((M,M,M,K,K,K)).byte()\n",
    "for i, j, k in product(range(M), repeat=3):\n",
    "    if len(set((i,j,k))) < 3:\n",
    "        mask[i,j,k,:,:,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 ms, sys: 980 µs, total: 13.1 ms\n",
      "Wall time: 3.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = np.einsum('aby,cdy,efy,y->acebdf', alpha, alpha, alpha, p_Y)\n",
    "O = torch.from_numpy(O).float()\n",
    "O[1-mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed labeling rates\n",
    "O_l = torch.from_numpy(np.einsum('aby,y->ab', alpha, p_Y)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Try to recover $\\alpha$ given $P(Y=y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(A, P, O, O_l):\n",
    "    \n",
    "    # Main constraint: match empirical pairwise overlaps matrix (entries O_ij for i != j)\n",
    "    loss_1 = torch.norm((O - torch.einsum('aby,cdy,efy,y->acebdf', [A,A,A,P]))[mask])**2\n",
    "    \n",
    "    # Col-wise stochastic: \\sum_y' P(\\lf=y'|Y=y) = 1.0\n",
    "    loss_2 = torch.norm(torch.sum(A, 1) - 1)**2\n",
    "    \n",
    "    # Row-wise constraint: match observed labeling rates P(\\lf=y') = \\sum_y P(Y=y) P(\\lf=y'|Y=y)\n",
    "    loss_3 = torch.norm(O_l - torch.einsum('aby,y->ab', [A,P]))**2\n",
    "    \n",
    "    return loss_1 + loss_2 + loss_3\n",
    "\n",
    "def train_model(A, P, O, O_l, n_epochs=10, lr=0.01, print_every=1):\n",
    "    optimizer = optim.SGD([A], lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to calculate outputs\n",
    "        loss = get_loss(A, P, O, O_l)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep running sum of losses\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "        # Report progress\n",
    "        if epoch % print_every == 0:\n",
    "            msg = f\"[E:{epoch}]\\tLoss: {epoch_loss:.8f}\"\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E:0]\tLoss: 486.86047363\n",
      "[E:1000]\tLoss: 0.02988081\n",
      "[E:2000]\tLoss: 0.01138565\n",
      "[E:3000]\tLoss: 0.00566233\n",
      "[E:4000]\tLoss: 0.00378250\n",
      "[E:5000]\tLoss: 0.00243895\n",
      "[E:6000]\tLoss: 0.00145715\n",
      "[E:7000]\tLoss: 0.00080929\n",
      "[E:8000]\tLoss: 0.00042357\n",
      "[E:9000]\tLoss: 0.00021229\n",
      "Param estimation error: 0.0044183650801324715\n"
     ]
    }
   ],
   "source": [
    "A = nn.Parameter(torch.from_numpy(np.random.rand(M, K, K)).float()).float()\n",
    "P = torch.from_numpy(p_Y).float()\n",
    "\n",
    "train_model(A, P, O, O_l, n_epochs=10000, lr=0.01, print_every=1000)\n",
    "\n",
    "print(f\"Param estimation error: {np.mean(np.abs(A.detach().numpy() - alpha))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2820, 0.2384, 0.3995],\n",
       "        [0.3665, 0.3889, 0.3117],\n",
       "        [0.3516, 0.3726, 0.2887]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28359862, 0.23194285, 0.39931097],\n",
       "       [0.36570757, 0.39197016, 0.31187131],\n",
       "       [0.35069381, 0.376087  , 0.28881771]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9199, 1.0672, 1.0129], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91485244, 1.06954904, 1.01559852])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha[0].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (MeTaL)",
   "language": "python",
   "name": "metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
