{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal abstraction: the user specifies some \"frequency\" (in seconds, examples, batches, or epochs) at which to evaluate on dev set. Some arbitrary set of metrics (some built-in, some user-provided) are evaluated at that time (on the whole dev set or sum max number of examples). If checkpointing is on, the user specified which metric to use and whether to min or max it. If a new best is found, execute checkpointer to save model.\n",
    "\n",
    "Train loss gets reported continuously with tqdm progress bar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_config = {\n",
    "    'log_unit': 'epochs', # ['seconds', 'examples', 'batches', 'epochs']\n",
    "    'log_freq': 1,\n",
    "    'log_print_freq': None, # Defaults to log_freq (turned off by verbose)\n",
    "    'log_metrics': ['accuracy'], # Can include built-in and user-defined metrics\n",
    "    'validation_freq': 1,\n",
    "    'validation_print_freq': None, # Defaults to validation_freq (turned off by verbose)\n",
    "    'validation_metrics': None, # Must be in log_metrics\n",
    "    'validation_mode': 'max', # ['max', 'min']\n",
    "    'validation_max_examples': None, # If not-None, only validate on this many examples (e.g., for speed)\n",
    "    'tensorboard': False,\n",
    "    'tensorboard_config': { # Event file stored at log_dir/run_dir/run_name (see slicing)\n",
    "        'log_dir': 'tensorboard',\n",
    "        'run_dir': None,\n",
    "        'run_name': None,\n",
    "        'tb_metrics': None, # Must be a subset of log_metrics; defaults to all of them\n",
    "    }\n",
    "}\n",
    "checkpoint_config = {\n",
    "    'checkpoint': True, # If True, save best model\n",
    "    'checkpoint_metric': 'accuracy', # Must be in either log_metrics or validation_metrics\n",
    "    'checkpointer_config': {\n",
    "        'checkpoint_dir': 'checkpoints',\n",
    "        'checkpoint_runway': 0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0051357746124268\n",
      "1.000446081161499\n",
      "1.0028727054595947\n",
      "3.008953809738159\n"
     ]
    }
   ],
   "source": [
    "from metal.logging import Timer\n",
    "import time\n",
    "\n",
    "timer = Timer()\n",
    "for x in range(3):\n",
    "    time.sleep(1)\n",
    "    print(timer.elapsed())\n",
    "print(timer.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "\n",
    "em = EndModel([2,10,2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[23 seconds]: accuracy=0.835, epochs=3,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
